{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3de0641e",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_filepath=\"/data/workdirs/bevfusion-multitask/dgbev1/epoch_6.pth\"\n",
    "seg_filepath=\"/data/workdirs/bevfusion-map-seg/baseline_dgbev/epoch_20.pth\"\n",
    "det_filepath=\"/data/workdirs/bevfusion/depth_map5/epoch_6.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3ef69050",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "10fda56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_pth=torch.load(multi_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6e09fe5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['meta', 'state_dict', 'optimizer'])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_pth.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4c80cd3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['encoders.camera.backbone.patch_embed.projection.weight', 'encoders.camera.backbone.patch_embed.projection.bias', 'encoders.camera.backbone.patch_embed.norm.weight', 'encoders.camera.backbone.patch_embed.norm.bias', 'encoders.camera.backbone.stages.0.blocks.0.norm1.weight', 'encoders.camera.backbone.stages.0.blocks.0.norm1.bias', 'encoders.camera.backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table', 'encoders.camera.backbone.stages.0.blocks.0.attn.w_msa.relative_position_index', 'encoders.camera.backbone.stages.0.blocks.0.attn.w_msa.qkv.weight', 'encoders.camera.backbone.stages.0.blocks.0.attn.w_msa.qkv.bias', 'encoders.camera.backbone.stages.0.blocks.0.attn.w_msa.proj.weight', 'encoders.camera.backbone.stages.0.blocks.0.attn.w_msa.proj.bias', 'encoders.camera.backbone.stages.0.blocks.0.norm2.weight', 'encoders.camera.backbone.stages.0.blocks.0.norm2.bias', 'encoders.camera.backbone.stages.0.blocks.0.ffn.layers.0.0.weight', 'encoders.camera.backbone.stages.0.blocks.0.ffn.layers.0.0.bias', 'encoders.camera.backbone.stages.0.blocks.0.ffn.layers.1.weight', 'encoders.camera.backbone.stages.0.blocks.0.ffn.layers.1.bias', 'encoders.camera.backbone.stages.0.blocks.1.norm1.weight', 'encoders.camera.backbone.stages.0.blocks.1.norm1.bias', 'encoders.camera.backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table', 'encoders.camera.backbone.stages.0.blocks.1.attn.w_msa.relative_position_index', 'encoders.camera.backbone.stages.0.blocks.1.attn.w_msa.qkv.weight', 'encoders.camera.backbone.stages.0.blocks.1.attn.w_msa.qkv.bias', 'encoders.camera.backbone.stages.0.blocks.1.attn.w_msa.proj.weight', 'encoders.camera.backbone.stages.0.blocks.1.attn.w_msa.proj.bias', 'encoders.camera.backbone.stages.0.blocks.1.norm2.weight', 'encoders.camera.backbone.stages.0.blocks.1.norm2.bias', 'encoders.camera.backbone.stages.0.blocks.1.ffn.layers.0.0.weight', 'encoders.camera.backbone.stages.0.blocks.1.ffn.layers.0.0.bias', 'encoders.camera.backbone.stages.0.blocks.1.ffn.layers.1.weight', 'encoders.camera.backbone.stages.0.blocks.1.ffn.layers.1.bias', 'encoders.camera.backbone.stages.0.downsample.norm.weight', 'encoders.camera.backbone.stages.0.downsample.norm.bias', 'encoders.camera.backbone.stages.0.downsample.reduction.weight', 'encoders.camera.backbone.stages.1.blocks.0.norm1.weight', 'encoders.camera.backbone.stages.1.blocks.0.norm1.bias', 'encoders.camera.backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table', 'encoders.camera.backbone.stages.1.blocks.0.attn.w_msa.relative_position_index', 'encoders.camera.backbone.stages.1.blocks.0.attn.w_msa.qkv.weight', 'encoders.camera.backbone.stages.1.blocks.0.attn.w_msa.qkv.bias', 'encoders.camera.backbone.stages.1.blocks.0.attn.w_msa.proj.weight', 'encoders.camera.backbone.stages.1.blocks.0.attn.w_msa.proj.bias', 'encoders.camera.backbone.stages.1.blocks.0.norm2.weight', 'encoders.camera.backbone.stages.1.blocks.0.norm2.bias', 'encoders.camera.backbone.stages.1.blocks.0.ffn.layers.0.0.weight', 'encoders.camera.backbone.stages.1.blocks.0.ffn.layers.0.0.bias', 'encoders.camera.backbone.stages.1.blocks.0.ffn.layers.1.weight', 'encoders.camera.backbone.stages.1.blocks.0.ffn.layers.1.bias', 'encoders.camera.backbone.stages.1.blocks.1.norm1.weight', 'encoders.camera.backbone.stages.1.blocks.1.norm1.bias', 'encoders.camera.backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table', 'encoders.camera.backbone.stages.1.blocks.1.attn.w_msa.relative_position_index', 'encoders.camera.backbone.stages.1.blocks.1.attn.w_msa.qkv.weight', 'encoders.camera.backbone.stages.1.blocks.1.attn.w_msa.qkv.bias', 'encoders.camera.backbone.stages.1.blocks.1.attn.w_msa.proj.weight', 'encoders.camera.backbone.stages.1.blocks.1.attn.w_msa.proj.bias', 'encoders.camera.backbone.stages.1.blocks.1.norm2.weight', 'encoders.camera.backbone.stages.1.blocks.1.norm2.bias', 'encoders.camera.backbone.stages.1.blocks.1.ffn.layers.0.0.weight', 'encoders.camera.backbone.stages.1.blocks.1.ffn.layers.0.0.bias', 'encoders.camera.backbone.stages.1.blocks.1.ffn.layers.1.weight', 'encoders.camera.backbone.stages.1.blocks.1.ffn.layers.1.bias', 'encoders.camera.backbone.stages.1.downsample.norm.weight', 'encoders.camera.backbone.stages.1.downsample.norm.bias', 'encoders.camera.backbone.stages.1.downsample.reduction.weight', 'encoders.camera.backbone.stages.2.blocks.0.norm1.weight', 'encoders.camera.backbone.stages.2.blocks.0.norm1.bias', 'encoders.camera.backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table', 'encoders.camera.backbone.stages.2.blocks.0.attn.w_msa.relative_position_index', 'encoders.camera.backbone.stages.2.blocks.0.attn.w_msa.qkv.weight', 'encoders.camera.backbone.stages.2.blocks.0.attn.w_msa.qkv.bias', 'encoders.camera.backbone.stages.2.blocks.0.attn.w_msa.proj.weight', 'encoders.camera.backbone.stages.2.blocks.0.attn.w_msa.proj.bias', 'encoders.camera.backbone.stages.2.blocks.0.norm2.weight', 'encoders.camera.backbone.stages.2.blocks.0.norm2.bias', 'encoders.camera.backbone.stages.2.blocks.0.ffn.layers.0.0.weight', 'encoders.camera.backbone.stages.2.blocks.0.ffn.layers.0.0.bias', 'encoders.camera.backbone.stages.2.blocks.0.ffn.layers.1.weight', 'encoders.camera.backbone.stages.2.blocks.0.ffn.layers.1.bias', 'encoders.camera.backbone.stages.2.blocks.1.norm1.weight', 'encoders.camera.backbone.stages.2.blocks.1.norm1.bias', 'encoders.camera.backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table', 'encoders.camera.backbone.stages.2.blocks.1.attn.w_msa.relative_position_index', 'encoders.camera.backbone.stages.2.blocks.1.attn.w_msa.qkv.weight', 'encoders.camera.backbone.stages.2.blocks.1.attn.w_msa.qkv.bias', 'encoders.camera.backbone.stages.2.blocks.1.attn.w_msa.proj.weight', 'encoders.camera.backbone.stages.2.blocks.1.attn.w_msa.proj.bias', 'encoders.camera.backbone.stages.2.blocks.1.norm2.weight', 'encoders.camera.backbone.stages.2.blocks.1.norm2.bias', 'encoders.camera.backbone.stages.2.blocks.1.ffn.layers.0.0.weight', 'encoders.camera.backbone.stages.2.blocks.1.ffn.layers.0.0.bias', 'encoders.camera.backbone.stages.2.blocks.1.ffn.layers.1.weight', 'encoders.camera.backbone.stages.2.blocks.1.ffn.layers.1.bias', 'encoders.camera.backbone.stages.2.blocks.2.norm1.weight', 'encoders.camera.backbone.stages.2.blocks.2.norm1.bias', 'encoders.camera.backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table', 'encoders.camera.backbone.stages.2.blocks.2.attn.w_msa.relative_position_index', 'encoders.camera.backbone.stages.2.blocks.2.attn.w_msa.qkv.weight', 'encoders.camera.backbone.stages.2.blocks.2.attn.w_msa.qkv.bias', 'encoders.camera.backbone.stages.2.blocks.2.attn.w_msa.proj.weight', 'encoders.camera.backbone.stages.2.blocks.2.attn.w_msa.proj.bias', 'encoders.camera.backbone.stages.2.blocks.2.norm2.weight', 'encoders.camera.backbone.stages.2.blocks.2.norm2.bias', 'encoders.camera.backbone.stages.2.blocks.2.ffn.layers.0.0.weight', 'encoders.camera.backbone.stages.2.blocks.2.ffn.layers.0.0.bias', 'encoders.camera.backbone.stages.2.blocks.2.ffn.layers.1.weight', 'encoders.camera.backbone.stages.2.blocks.2.ffn.layers.1.bias', 'encoders.camera.backbone.stages.2.blocks.3.norm1.weight', 'encoders.camera.backbone.stages.2.blocks.3.norm1.bias', 'encoders.camera.backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table', 'encoders.camera.backbone.stages.2.blocks.3.attn.w_msa.relative_position_index', 'encoders.camera.backbone.stages.2.blocks.3.attn.w_msa.qkv.weight', 'encoders.camera.backbone.stages.2.blocks.3.attn.w_msa.qkv.bias', 'encoders.camera.backbone.stages.2.blocks.3.attn.w_msa.proj.weight', 'encoders.camera.backbone.stages.2.blocks.3.attn.w_msa.proj.bias', 'encoders.camera.backbone.stages.2.blocks.3.norm2.weight', 'encoders.camera.backbone.stages.2.blocks.3.norm2.bias', 'encoders.camera.backbone.stages.2.blocks.3.ffn.layers.0.0.weight', 'encoders.camera.backbone.stages.2.blocks.3.ffn.layers.0.0.bias', 'encoders.camera.backbone.stages.2.blocks.3.ffn.layers.1.weight', 'encoders.camera.backbone.stages.2.blocks.3.ffn.layers.1.bias', 'encoders.camera.backbone.stages.2.blocks.4.norm1.weight', 'encoders.camera.backbone.stages.2.blocks.4.norm1.bias', 'encoders.camera.backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table', 'encoders.camera.backbone.stages.2.blocks.4.attn.w_msa.relative_position_index', 'encoders.camera.backbone.stages.2.blocks.4.attn.w_msa.qkv.weight', 'encoders.camera.backbone.stages.2.blocks.4.attn.w_msa.qkv.bias', 'encoders.camera.backbone.stages.2.blocks.4.attn.w_msa.proj.weight', 'encoders.camera.backbone.stages.2.blocks.4.attn.w_msa.proj.bias', 'encoders.camera.backbone.stages.2.blocks.4.norm2.weight', 'encoders.camera.backbone.stages.2.blocks.4.norm2.bias', 'encoders.camera.backbone.stages.2.blocks.4.ffn.layers.0.0.weight', 'encoders.camera.backbone.stages.2.blocks.4.ffn.layers.0.0.bias', 'encoders.camera.backbone.stages.2.blocks.4.ffn.layers.1.weight', 'encoders.camera.backbone.stages.2.blocks.4.ffn.layers.1.bias', 'encoders.camera.backbone.stages.2.blocks.5.norm1.weight', 'encoders.camera.backbone.stages.2.blocks.5.norm1.bias', 'encoders.camera.backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table', 'encoders.camera.backbone.stages.2.blocks.5.attn.w_msa.relative_position_index', 'encoders.camera.backbone.stages.2.blocks.5.attn.w_msa.qkv.weight', 'encoders.camera.backbone.stages.2.blocks.5.attn.w_msa.qkv.bias', 'encoders.camera.backbone.stages.2.blocks.5.attn.w_msa.proj.weight', 'encoders.camera.backbone.stages.2.blocks.5.attn.w_msa.proj.bias', 'encoders.camera.backbone.stages.2.blocks.5.norm2.weight', 'encoders.camera.backbone.stages.2.blocks.5.norm2.bias', 'encoders.camera.backbone.stages.2.blocks.5.ffn.layers.0.0.weight', 'encoders.camera.backbone.stages.2.blocks.5.ffn.layers.0.0.bias', 'encoders.camera.backbone.stages.2.blocks.5.ffn.layers.1.weight', 'encoders.camera.backbone.stages.2.blocks.5.ffn.layers.1.bias', 'encoders.camera.backbone.stages.2.downsample.norm.weight', 'encoders.camera.backbone.stages.2.downsample.norm.bias', 'encoders.camera.backbone.stages.2.downsample.reduction.weight', 'encoders.camera.backbone.stages.3.blocks.0.norm1.weight', 'encoders.camera.backbone.stages.3.blocks.0.norm1.bias', 'encoders.camera.backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table', 'encoders.camera.backbone.stages.3.blocks.0.attn.w_msa.relative_position_index', 'encoders.camera.backbone.stages.3.blocks.0.attn.w_msa.qkv.weight', 'encoders.camera.backbone.stages.3.blocks.0.attn.w_msa.qkv.bias', 'encoders.camera.backbone.stages.3.blocks.0.attn.w_msa.proj.weight', 'encoders.camera.backbone.stages.3.blocks.0.attn.w_msa.proj.bias', 'encoders.camera.backbone.stages.3.blocks.0.norm2.weight', 'encoders.camera.backbone.stages.3.blocks.0.norm2.bias', 'encoders.camera.backbone.stages.3.blocks.0.ffn.layers.0.0.weight', 'encoders.camera.backbone.stages.3.blocks.0.ffn.layers.0.0.bias', 'encoders.camera.backbone.stages.3.blocks.0.ffn.layers.1.weight', 'encoders.camera.backbone.stages.3.blocks.0.ffn.layers.1.bias', 'encoders.camera.backbone.stages.3.blocks.1.norm1.weight', 'encoders.camera.backbone.stages.3.blocks.1.norm1.bias', 'encoders.camera.backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table', 'encoders.camera.backbone.stages.3.blocks.1.attn.w_msa.relative_position_index', 'encoders.camera.backbone.stages.3.blocks.1.attn.w_msa.qkv.weight', 'encoders.camera.backbone.stages.3.blocks.1.attn.w_msa.qkv.bias', 'encoders.camera.backbone.stages.3.blocks.1.attn.w_msa.proj.weight', 'encoders.camera.backbone.stages.3.blocks.1.attn.w_msa.proj.bias', 'encoders.camera.backbone.stages.3.blocks.1.norm2.weight', 'encoders.camera.backbone.stages.3.blocks.1.norm2.bias', 'encoders.camera.backbone.stages.3.blocks.1.ffn.layers.0.0.weight', 'encoders.camera.backbone.stages.3.blocks.1.ffn.layers.0.0.bias', 'encoders.camera.backbone.stages.3.blocks.1.ffn.layers.1.weight', 'encoders.camera.backbone.stages.3.blocks.1.ffn.layers.1.bias', 'encoders.camera.backbone.norm1.weight', 'encoders.camera.backbone.norm1.bias', 'encoders.camera.backbone.norm2.weight', 'encoders.camera.backbone.norm2.bias', 'encoders.camera.backbone.norm3.weight', 'encoders.camera.backbone.norm3.bias', 'encoders.camera.neck.lateral_convs.0.conv.weight', 'encoders.camera.neck.lateral_convs.0.bn.weight', 'encoders.camera.neck.lateral_convs.0.bn.bias', 'encoders.camera.neck.lateral_convs.0.bn.running_mean', 'encoders.camera.neck.lateral_convs.0.bn.running_var', 'encoders.camera.neck.lateral_convs.0.bn.num_batches_tracked', 'encoders.camera.neck.lateral_convs.1.conv.weight', 'encoders.camera.neck.lateral_convs.1.bn.weight', 'encoders.camera.neck.lateral_convs.1.bn.bias', 'encoders.camera.neck.lateral_convs.1.bn.running_mean', 'encoders.camera.neck.lateral_convs.1.bn.running_var', 'encoders.camera.neck.lateral_convs.1.bn.num_batches_tracked', 'encoders.camera.neck.fpn_convs.0.conv.weight', 'encoders.camera.neck.fpn_convs.0.bn.weight', 'encoders.camera.neck.fpn_convs.0.bn.bias', 'encoders.camera.neck.fpn_convs.0.bn.running_mean', 'encoders.camera.neck.fpn_convs.0.bn.running_var', 'encoders.camera.neck.fpn_convs.0.bn.num_batches_tracked', 'encoders.camera.neck.fpn_convs.1.conv.weight', 'encoders.camera.neck.fpn_convs.1.bn.weight', 'encoders.camera.neck.fpn_convs.1.bn.bias', 'encoders.camera.neck.fpn_convs.1.bn.running_mean', 'encoders.camera.neck.fpn_convs.1.bn.running_var', 'encoders.camera.neck.fpn_convs.1.bn.num_batches_tracked', 'encoders.camera.vtransform.dx', 'encoders.camera.vtransform.bx', 'encoders.camera.vtransform.nx', 'encoders.camera.vtransform.frustum', 'encoders.camera.vtransform.dtransform.0.weight', 'encoders.camera.vtransform.dtransform.0.bias', 'encoders.camera.vtransform.dtransform.1.weight', 'encoders.camera.vtransform.dtransform.1.bias', 'encoders.camera.vtransform.dtransform.1.running_mean', 'encoders.camera.vtransform.dtransform.1.running_var', 'encoders.camera.vtransform.dtransform.1.num_batches_tracked', 'encoders.camera.vtransform.dtransform.3.weight', 'encoders.camera.vtransform.dtransform.3.bias', 'encoders.camera.vtransform.dtransform.4.weight', 'encoders.camera.vtransform.dtransform.4.bias', 'encoders.camera.vtransform.dtransform.4.running_mean', 'encoders.camera.vtransform.dtransform.4.running_var', 'encoders.camera.vtransform.dtransform.4.num_batches_tracked', 'encoders.camera.vtransform.dtransform.6.weight', 'encoders.camera.vtransform.dtransform.6.bias', 'encoders.camera.vtransform.dtransform.7.weight', 'encoders.camera.vtransform.dtransform.7.bias', 'encoders.camera.vtransform.dtransform.7.running_mean', 'encoders.camera.vtransform.dtransform.7.running_var', 'encoders.camera.vtransform.dtransform.7.num_batches_tracked', 'encoders.camera.vtransform.depth_map_transform.0.weight', 'encoders.camera.vtransform.depth_map_transform.0.bias', 'encoders.camera.vtransform.depth_map_transform.1.weight', 'encoders.camera.vtransform.depth_map_transform.1.bias', 'encoders.camera.vtransform.depth_map_transform.1.running_mean', 'encoders.camera.vtransform.depth_map_transform.1.running_var', 'encoders.camera.vtransform.depth_map_transform.1.num_batches_tracked', 'encoders.camera.vtransform.depth_map_transform.3.weight', 'encoders.camera.vtransform.depth_map_transform.3.bias', 'encoders.camera.vtransform.prenet.0.weight', 'encoders.camera.vtransform.prenet.0.bias', 'encoders.camera.vtransform.prenet.1.weight', 'encoders.camera.vtransform.prenet.1.bias', 'encoders.camera.vtransform.prenet.1.running_mean', 'encoders.camera.vtransform.prenet.1.running_var', 'encoders.camera.vtransform.prenet.1.num_batches_tracked', 'encoders.camera.vtransform.prenet.3.weight', 'encoders.camera.vtransform.prenet.3.bias', 'encoders.camera.vtransform.prenet.4.weight', 'encoders.camera.vtransform.prenet.4.bias', 'encoders.camera.vtransform.prenet.4.running_mean', 'encoders.camera.vtransform.prenet.4.running_var', 'encoders.camera.vtransform.prenet.4.num_batches_tracked', 'encoders.camera.vtransform.depthnet.weight', 'encoders.camera.vtransform.depthnet.bias', 'encoders.camera.vtransform.contextnet.weight', 'encoders.camera.vtransform.contextnet.bias', 'encoders.camera.vtransform.attention1.conv.weight', 'encoders.camera.vtransform.attention1.conv.bias', 'encoders.camera.vtransform.attention1.channel_attention.fc.0.weight', 'encoders.camera.vtransform.attention1.channel_attention.fc.0.bias', 'encoders.camera.vtransform.attention1.channel_attention.fc.2.weight', 'encoders.camera.vtransform.attention1.channel_attention.fc.2.bias', 'encoders.camera.vtransform.convnet.0.weight', 'encoders.camera.vtransform.convnet.0.bias', 'encoders.camera.vtransform.convnet.1.weight', 'encoders.camera.vtransform.convnet.1.bias', 'encoders.camera.vtransform.convnet.1.running_mean', 'encoders.camera.vtransform.convnet.1.running_var', 'encoders.camera.vtransform.convnet.1.num_batches_tracked', 'encoders.camera.vtransform.convnet.3.weight', 'encoders.camera.vtransform.convnet.3.bias', 'encoders.camera.vtransform.convnet.4.weight', 'encoders.camera.vtransform.convnet.4.bias', 'encoders.camera.vtransform.convnet.4.running_mean', 'encoders.camera.vtransform.convnet.4.running_var', 'encoders.camera.vtransform.convnet.4.num_batches_tracked', 'encoders.camera.vtransform.downsample.0.weight', 'encoders.camera.vtransform.downsample.1.weight', 'encoders.camera.vtransform.downsample.1.bias', 'encoders.camera.vtransform.downsample.1.running_mean', 'encoders.camera.vtransform.downsample.1.running_var', 'encoders.camera.vtransform.downsample.1.num_batches_tracked', 'encoders.camera.vtransform.downsample.3.weight', 'encoders.camera.vtransform.downsample.4.weight', 'encoders.camera.vtransform.downsample.4.bias', 'encoders.camera.vtransform.downsample.4.running_mean', 'encoders.camera.vtransform.downsample.4.running_var', 'encoders.camera.vtransform.downsample.4.num_batches_tracked', 'encoders.camera.vtransform.downsample.6.weight', 'encoders.camera.vtransform.downsample.7.weight', 'encoders.camera.vtransform.downsample.7.bias', 'encoders.camera.vtransform.downsample.7.running_mean', 'encoders.camera.vtransform.downsample.7.running_var', 'encoders.camera.vtransform.downsample.7.num_batches_tracked', 'encoders.lidar.backbone.conv_input.0.weight', 'encoders.lidar.backbone.conv_input.1.weight', 'encoders.lidar.backbone.conv_input.1.bias', 'encoders.lidar.backbone.conv_input.1.running_mean', 'encoders.lidar.backbone.conv_input.1.running_var', 'encoders.lidar.backbone.conv_input.1.num_batches_tracked', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.0.conv1.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.0.bn1.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.0.bn1.bias', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.0.bn1.running_mean', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.0.bn1.running_var', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.0.bn1.num_batches_tracked', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.0.conv2.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.0.bn2.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.0.bn2.bias', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.0.bn2.running_mean', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.0.bn2.running_var', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.0.bn2.num_batches_tracked', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.1.conv1.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.1.bn1.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.1.bn1.bias', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.1.bn1.running_mean', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.1.bn1.running_var', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.1.bn1.num_batches_tracked', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.1.conv2.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.1.bn2.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.1.bn2.bias', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.1.bn2.running_mean', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.1.bn2.running_var', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.1.bn2.num_batches_tracked', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.2.0.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.2.1.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.2.1.bias', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.2.1.running_mean', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.2.1.running_var', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.2.1.num_batches_tracked', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.0.conv1.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.0.bn1.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.0.bn1.bias', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.0.bn1.running_mean', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.0.bn1.running_var', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.0.bn1.num_batches_tracked', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.0.conv2.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.0.bn2.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.0.bn2.bias', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.0.bn2.running_mean', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.0.bn2.running_var', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.0.bn2.num_batches_tracked', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.1.conv1.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.1.bn1.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.1.bn1.bias', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.1.bn1.running_mean', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.1.bn1.running_var', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.1.bn1.num_batches_tracked', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.1.conv2.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.1.bn2.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.1.bn2.bias', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.1.bn2.running_mean', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.1.bn2.running_var', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.1.bn2.num_batches_tracked', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.2.0.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.2.1.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.2.1.bias', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.2.1.running_mean', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.2.1.running_var', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.2.1.num_batches_tracked', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.0.conv1.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.0.bn1.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.0.bn1.bias', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.0.bn1.running_mean', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.0.bn1.running_var', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.0.bn1.num_batches_tracked', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.0.conv2.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.0.bn2.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.0.bn2.bias', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.0.bn2.running_mean', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.0.bn2.running_var', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.0.bn2.num_batches_tracked', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.1.conv1.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.1.bn1.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.1.bn1.bias', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.1.bn1.running_mean', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.1.bn1.running_var', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.1.bn1.num_batches_tracked', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.1.conv2.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.1.bn2.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.1.bn2.bias', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.1.bn2.running_mean', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.1.bn2.running_var', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.1.bn2.num_batches_tracked', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.2.0.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.2.1.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.2.1.bias', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.2.1.running_mean', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.2.1.running_var', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.2.1.num_batches_tracked', 'encoders.lidar.backbone.encoder_layers.encoder_layer4.0.conv1.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer4.0.bn1.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer4.0.bn1.bias', 'encoders.lidar.backbone.encoder_layers.encoder_layer4.0.bn1.running_mean', 'encoders.lidar.backbone.encoder_layers.encoder_layer4.0.bn1.running_var', 'encoders.lidar.backbone.encoder_layers.encoder_layer4.0.bn1.num_batches_tracked', 'encoders.lidar.backbone.encoder_layers.encoder_layer4.0.conv2.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer4.0.bn2.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer4.0.bn2.bias', 'encoders.lidar.backbone.encoder_layers.encoder_layer4.0.bn2.running_mean', 'encoders.lidar.backbone.encoder_layers.encoder_layer4.0.bn2.running_var', 'encoders.lidar.backbone.encoder_layers.encoder_layer4.0.bn2.num_batches_tracked', 'encoders.lidar.backbone.encoder_layers.encoder_layer4.1.conv1.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer4.1.bn1.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer4.1.bn1.bias', 'encoders.lidar.backbone.encoder_layers.encoder_layer4.1.bn1.running_mean', 'encoders.lidar.backbone.encoder_layers.encoder_layer4.1.bn1.running_var', 'encoders.lidar.backbone.encoder_layers.encoder_layer4.1.bn1.num_batches_tracked', 'encoders.lidar.backbone.encoder_layers.encoder_layer4.1.conv2.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer4.1.bn2.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer4.1.bn2.bias', 'encoders.lidar.backbone.encoder_layers.encoder_layer4.1.bn2.running_mean', 'encoders.lidar.backbone.encoder_layers.encoder_layer4.1.bn2.running_var', 'encoders.lidar.backbone.encoder_layers.encoder_layer4.1.bn2.num_batches_tracked', 'encoders.lidar.backbone.conv_out.0.weight', 'encoders.lidar.backbone.conv_out.1.weight', 'encoders.lidar.backbone.conv_out.1.bias', 'encoders.lidar.backbone.conv_out.1.running_mean', 'encoders.lidar.backbone.conv_out.1.running_var', 'encoders.lidar.backbone.conv_out.1.num_batches_tracked', 'fuser.0.weight', 'fuser.1.weight', 'fuser.1.bias', 'fuser.1.running_mean', 'fuser.1.running_var', 'fuser.1.num_batches_tracked', 'decoder.backbone.blocks.0.0.weight', 'decoder.backbone.blocks.0.1.weight', 'decoder.backbone.blocks.0.1.bias', 'decoder.backbone.blocks.0.1.running_mean', 'decoder.backbone.blocks.0.1.running_var', 'decoder.backbone.blocks.0.1.num_batches_tracked', 'decoder.backbone.blocks.0.3.weight', 'decoder.backbone.blocks.0.4.weight', 'decoder.backbone.blocks.0.4.bias', 'decoder.backbone.blocks.0.4.running_mean', 'decoder.backbone.blocks.0.4.running_var', 'decoder.backbone.blocks.0.4.num_batches_tracked', 'decoder.backbone.blocks.0.6.weight', 'decoder.backbone.blocks.0.7.weight', 'decoder.backbone.blocks.0.7.bias', 'decoder.backbone.blocks.0.7.running_mean', 'decoder.backbone.blocks.0.7.running_var', 'decoder.backbone.blocks.0.7.num_batches_tracked', 'decoder.backbone.blocks.0.9.weight', 'decoder.backbone.blocks.0.10.weight', 'decoder.backbone.blocks.0.10.bias', 'decoder.backbone.blocks.0.10.running_mean', 'decoder.backbone.blocks.0.10.running_var', 'decoder.backbone.blocks.0.10.num_batches_tracked', 'decoder.backbone.blocks.0.12.weight', 'decoder.backbone.blocks.0.13.weight', 'decoder.backbone.blocks.0.13.bias', 'decoder.backbone.blocks.0.13.running_mean', 'decoder.backbone.blocks.0.13.running_var', 'decoder.backbone.blocks.0.13.num_batches_tracked', 'decoder.backbone.blocks.0.15.weight', 'decoder.backbone.blocks.0.16.weight', 'decoder.backbone.blocks.0.16.bias', 'decoder.backbone.blocks.0.16.running_mean', 'decoder.backbone.blocks.0.16.running_var', 'decoder.backbone.blocks.0.16.num_batches_tracked', 'decoder.backbone.blocks.1.0.weight', 'decoder.backbone.blocks.1.1.weight', 'decoder.backbone.blocks.1.1.bias', 'decoder.backbone.blocks.1.1.running_mean', 'decoder.backbone.blocks.1.1.running_var', 'decoder.backbone.blocks.1.1.num_batches_tracked', 'decoder.backbone.blocks.1.3.weight', 'decoder.backbone.blocks.1.4.weight', 'decoder.backbone.blocks.1.4.bias', 'decoder.backbone.blocks.1.4.running_mean', 'decoder.backbone.blocks.1.4.running_var', 'decoder.backbone.blocks.1.4.num_batches_tracked', 'decoder.backbone.blocks.1.6.weight', 'decoder.backbone.blocks.1.7.weight', 'decoder.backbone.blocks.1.7.bias', 'decoder.backbone.blocks.1.7.running_mean', 'decoder.backbone.blocks.1.7.running_var', 'decoder.backbone.blocks.1.7.num_batches_tracked', 'decoder.backbone.blocks.1.9.weight', 'decoder.backbone.blocks.1.10.weight', 'decoder.backbone.blocks.1.10.bias', 'decoder.backbone.blocks.1.10.running_mean', 'decoder.backbone.blocks.1.10.running_var', 'decoder.backbone.blocks.1.10.num_batches_tracked', 'decoder.backbone.blocks.1.12.weight', 'decoder.backbone.blocks.1.13.weight', 'decoder.backbone.blocks.1.13.bias', 'decoder.backbone.blocks.1.13.running_mean', 'decoder.backbone.blocks.1.13.running_var', 'decoder.backbone.blocks.1.13.num_batches_tracked', 'decoder.backbone.blocks.1.15.weight', 'decoder.backbone.blocks.1.16.weight', 'decoder.backbone.blocks.1.16.bias', 'decoder.backbone.blocks.1.16.running_mean', 'decoder.backbone.blocks.1.16.running_var', 'decoder.backbone.blocks.1.16.num_batches_tracked', 'decoder.neck.deblocks.0.0.weight', 'decoder.neck.deblocks.0.1.weight', 'decoder.neck.deblocks.0.1.bias', 'decoder.neck.deblocks.0.1.running_mean', 'decoder.neck.deblocks.0.1.running_var', 'decoder.neck.deblocks.0.1.num_batches_tracked', 'decoder.neck.deblocks.1.0.weight', 'decoder.neck.deblocks.1.1.weight', 'decoder.neck.deblocks.1.1.bias', 'decoder.neck.deblocks.1.1.running_mean', 'decoder.neck.deblocks.1.1.running_var', 'decoder.neck.deblocks.1.1.num_batches_tracked', 'heads.map.classifier.0.weight', 'heads.map.classifier.1.weight', 'heads.map.classifier.1.bias', 'heads.map.classifier.1.running_mean', 'heads.map.classifier.1.running_var', 'heads.map.classifier.1.num_batches_tracked', 'heads.map.classifier.3.weight', 'heads.map.classifier.4.weight', 'heads.map.classifier.4.bias', 'heads.map.classifier.4.running_mean', 'heads.map.classifier.4.running_var', 'heads.map.classifier.4.num_batches_tracked', 'heads.map.classifier.6.weight', 'heads.map.classifier.6.bias', 'heads.object.shared_conv.weight', 'heads.object.shared_conv.bias', 'heads.object.heatmap_head.0.conv.weight', 'heads.object.heatmap_head.0.bn.weight', 'heads.object.heatmap_head.0.bn.bias', 'heads.object.heatmap_head.0.bn.running_mean', 'heads.object.heatmap_head.0.bn.running_var', 'heads.object.heatmap_head.0.bn.num_batches_tracked', 'heads.object.heatmap_head.1.weight', 'heads.object.heatmap_head.1.bias', 'heads.object.class_encoding.weight', 'heads.object.class_encoding.bias', 'heads.object.decoder.0.self_attn.in_proj_weight', 'heads.object.decoder.0.self_attn.in_proj_bias', 'heads.object.decoder.0.self_attn.out_proj.weight', 'heads.object.decoder.0.self_attn.out_proj.bias', 'heads.object.decoder.0.multihead_attn.in_proj_weight', 'heads.object.decoder.0.multihead_attn.in_proj_bias', 'heads.object.decoder.0.multihead_attn.out_proj.weight', 'heads.object.decoder.0.multihead_attn.out_proj.bias', 'heads.object.decoder.0.linear1.weight', 'heads.object.decoder.0.linear1.bias', 'heads.object.decoder.0.linear2.weight', 'heads.object.decoder.0.linear2.bias', 'heads.object.decoder.0.norm1.weight', 'heads.object.decoder.0.norm1.bias', 'heads.object.decoder.0.norm2.weight', 'heads.object.decoder.0.norm2.bias', 'heads.object.decoder.0.norm3.weight', 'heads.object.decoder.0.norm3.bias', 'heads.object.decoder.0.self_posembed.position_embedding_head.0.weight', 'heads.object.decoder.0.self_posembed.position_embedding_head.0.bias', 'heads.object.decoder.0.self_posembed.position_embedding_head.1.weight', 'heads.object.decoder.0.self_posembed.position_embedding_head.1.bias', 'heads.object.decoder.0.self_posembed.position_embedding_head.1.running_mean', 'heads.object.decoder.0.self_posembed.position_embedding_head.1.running_var', 'heads.object.decoder.0.self_posembed.position_embedding_head.1.num_batches_tracked', 'heads.object.decoder.0.self_posembed.position_embedding_head.3.weight', 'heads.object.decoder.0.self_posembed.position_embedding_head.3.bias', 'heads.object.decoder.0.cross_posembed.position_embedding_head.0.weight', 'heads.object.decoder.0.cross_posembed.position_embedding_head.0.bias', 'heads.object.decoder.0.cross_posembed.position_embedding_head.1.weight', 'heads.object.decoder.0.cross_posembed.position_embedding_head.1.bias', 'heads.object.decoder.0.cross_posembed.position_embedding_head.1.running_mean', 'heads.object.decoder.0.cross_posembed.position_embedding_head.1.running_var', 'heads.object.decoder.0.cross_posembed.position_embedding_head.1.num_batches_tracked', 'heads.object.decoder.0.cross_posembed.position_embedding_head.3.weight', 'heads.object.decoder.0.cross_posembed.position_embedding_head.3.bias', 'heads.object.prediction_heads.0.center.0.conv.weight', 'heads.object.prediction_heads.0.center.0.bn.weight', 'heads.object.prediction_heads.0.center.0.bn.bias', 'heads.object.prediction_heads.0.center.0.bn.running_mean', 'heads.object.prediction_heads.0.center.0.bn.running_var', 'heads.object.prediction_heads.0.center.0.bn.num_batches_tracked', 'heads.object.prediction_heads.0.center.1.weight', 'heads.object.prediction_heads.0.center.1.bias', 'heads.object.prediction_heads.0.height.0.conv.weight', 'heads.object.prediction_heads.0.height.0.bn.weight', 'heads.object.prediction_heads.0.height.0.bn.bias', 'heads.object.prediction_heads.0.height.0.bn.running_mean', 'heads.object.prediction_heads.0.height.0.bn.running_var', 'heads.object.prediction_heads.0.height.0.bn.num_batches_tracked', 'heads.object.prediction_heads.0.height.1.weight', 'heads.object.prediction_heads.0.height.1.bias', 'heads.object.prediction_heads.0.dim.0.conv.weight', 'heads.object.prediction_heads.0.dim.0.bn.weight', 'heads.object.prediction_heads.0.dim.0.bn.bias', 'heads.object.prediction_heads.0.dim.0.bn.running_mean', 'heads.object.prediction_heads.0.dim.0.bn.running_var', 'heads.object.prediction_heads.0.dim.0.bn.num_batches_tracked', 'heads.object.prediction_heads.0.dim.1.weight', 'heads.object.prediction_heads.0.dim.1.bias', 'heads.object.prediction_heads.0.rot.0.conv.weight', 'heads.object.prediction_heads.0.rot.0.bn.weight', 'heads.object.prediction_heads.0.rot.0.bn.bias', 'heads.object.prediction_heads.0.rot.0.bn.running_mean', 'heads.object.prediction_heads.0.rot.0.bn.running_var', 'heads.object.prediction_heads.0.rot.0.bn.num_batches_tracked', 'heads.object.prediction_heads.0.rot.1.weight', 'heads.object.prediction_heads.0.rot.1.bias', 'heads.object.prediction_heads.0.vel.0.conv.weight', 'heads.object.prediction_heads.0.vel.0.bn.weight', 'heads.object.prediction_heads.0.vel.0.bn.bias', 'heads.object.prediction_heads.0.vel.0.bn.running_mean', 'heads.object.prediction_heads.0.vel.0.bn.running_var', 'heads.object.prediction_heads.0.vel.0.bn.num_batches_tracked', 'heads.object.prediction_heads.0.vel.1.weight', 'heads.object.prediction_heads.0.vel.1.bias', 'heads.object.prediction_heads.0.heatmap.0.conv.weight', 'heads.object.prediction_heads.0.heatmap.0.bn.weight', 'heads.object.prediction_heads.0.heatmap.0.bn.bias', 'heads.object.prediction_heads.0.heatmap.0.bn.running_mean', 'heads.object.prediction_heads.0.heatmap.0.bn.running_var', 'heads.object.prediction_heads.0.heatmap.0.bn.num_batches_tracked', 'heads.object.prediction_heads.0.heatmap.1.weight', 'heads.object.prediction_heads.0.heatmap.1.bias'])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_pth['state_dict'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "61869572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['fp16', 'hook_msgs', 'epoch', 'iter', 'mmcv_version', 'time'])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_pth['meta'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b652fae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['state', 'param_groups'])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_pth['optimizer'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "accf1686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([96, 3, 4, 4])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_pth['state_dict']['encoders.camera.backbone.patch_embed.projection.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "88973b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_key=multi_pth['state_dict'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6480d330",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_pth=torch.load(seg_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c3b71e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_key=multi_pth['state_dict'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "eb7def1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_key==seg_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a5905e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_key-multi_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3dd950d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi2seg_key=multi_key-seg_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "01c05aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoders.camera.vtransform.attention1.channel_attention.fc.0.bias',\n",
       " 'encoders.camera.vtransform.attention1.channel_attention.fc.0.weight',\n",
       " 'encoders.camera.vtransform.attention1.channel_attention.fc.2.bias',\n",
       " 'encoders.camera.vtransform.attention1.channel_attention.fc.2.weight',\n",
       " 'encoders.camera.vtransform.attention1.conv.bias',\n",
       " 'encoders.camera.vtransform.attention1.conv.weight',\n",
       " 'encoders.camera.vtransform.contextnet.bias',\n",
       " 'encoders.camera.vtransform.contextnet.weight',\n",
       " 'encoders.camera.vtransform.convnet.0.bias',\n",
       " 'encoders.camera.vtransform.convnet.0.weight',\n",
       " 'encoders.camera.vtransform.convnet.1.bias',\n",
       " 'encoders.camera.vtransform.convnet.1.num_batches_tracked',\n",
       " 'encoders.camera.vtransform.convnet.1.running_mean',\n",
       " 'encoders.camera.vtransform.convnet.1.running_var',\n",
       " 'encoders.camera.vtransform.convnet.1.weight',\n",
       " 'encoders.camera.vtransform.convnet.3.bias',\n",
       " 'encoders.camera.vtransform.convnet.3.weight',\n",
       " 'encoders.camera.vtransform.convnet.4.bias',\n",
       " 'encoders.camera.vtransform.convnet.4.num_batches_tracked',\n",
       " 'encoders.camera.vtransform.convnet.4.running_mean',\n",
       " 'encoders.camera.vtransform.convnet.4.running_var',\n",
       " 'encoders.camera.vtransform.convnet.4.weight',\n",
       " 'encoders.camera.vtransform.depth_map_transform.0.bias',\n",
       " 'encoders.camera.vtransform.depth_map_transform.0.weight',\n",
       " 'encoders.camera.vtransform.depth_map_transform.1.bias',\n",
       " 'encoders.camera.vtransform.depth_map_transform.1.num_batches_tracked',\n",
       " 'encoders.camera.vtransform.depth_map_transform.1.running_mean',\n",
       " 'encoders.camera.vtransform.depth_map_transform.1.running_var',\n",
       " 'encoders.camera.vtransform.depth_map_transform.1.weight',\n",
       " 'encoders.camera.vtransform.depth_map_transform.3.bias',\n",
       " 'encoders.camera.vtransform.depth_map_transform.3.weight',\n",
       " 'encoders.camera.vtransform.dtransform.0.bias',\n",
       " 'encoders.camera.vtransform.dtransform.0.weight',\n",
       " 'encoders.camera.vtransform.dtransform.1.bias',\n",
       " 'encoders.camera.vtransform.dtransform.1.num_batches_tracked',\n",
       " 'encoders.camera.vtransform.dtransform.1.running_mean',\n",
       " 'encoders.camera.vtransform.dtransform.1.running_var',\n",
       " 'encoders.camera.vtransform.dtransform.1.weight',\n",
       " 'encoders.camera.vtransform.dtransform.3.bias',\n",
       " 'encoders.camera.vtransform.dtransform.3.weight',\n",
       " 'encoders.camera.vtransform.dtransform.4.bias',\n",
       " 'encoders.camera.vtransform.dtransform.4.num_batches_tracked',\n",
       " 'encoders.camera.vtransform.dtransform.4.running_mean',\n",
       " 'encoders.camera.vtransform.dtransform.4.running_var',\n",
       " 'encoders.camera.vtransform.dtransform.4.weight',\n",
       " 'encoders.camera.vtransform.dtransform.6.bias',\n",
       " 'encoders.camera.vtransform.dtransform.6.weight',\n",
       " 'encoders.camera.vtransform.dtransform.7.bias',\n",
       " 'encoders.camera.vtransform.dtransform.7.num_batches_tracked',\n",
       " 'encoders.camera.vtransform.dtransform.7.running_mean',\n",
       " 'encoders.camera.vtransform.dtransform.7.running_var',\n",
       " 'encoders.camera.vtransform.dtransform.7.weight',\n",
       " 'encoders.camera.vtransform.prenet.0.bias',\n",
       " 'encoders.camera.vtransform.prenet.0.weight',\n",
       " 'encoders.camera.vtransform.prenet.1.bias',\n",
       " 'encoders.camera.vtransform.prenet.1.num_batches_tracked',\n",
       " 'encoders.camera.vtransform.prenet.1.running_mean',\n",
       " 'encoders.camera.vtransform.prenet.1.running_var',\n",
       " 'encoders.camera.vtransform.prenet.1.weight',\n",
       " 'encoders.camera.vtransform.prenet.3.bias',\n",
       " 'encoders.camera.vtransform.prenet.3.weight',\n",
       " 'encoders.camera.vtransform.prenet.4.bias',\n",
       " 'encoders.camera.vtransform.prenet.4.num_batches_tracked',\n",
       " 'encoders.camera.vtransform.prenet.4.running_mean',\n",
       " 'encoders.camera.vtransform.prenet.4.running_var',\n",
       " 'encoders.camera.vtransform.prenet.4.weight',\n",
       " 'heads.object.class_encoding.bias',\n",
       " 'heads.object.class_encoding.weight',\n",
       " 'heads.object.decoder.0.cross_posembed.position_embedding_head.0.bias',\n",
       " 'heads.object.decoder.0.cross_posembed.position_embedding_head.0.weight',\n",
       " 'heads.object.decoder.0.cross_posembed.position_embedding_head.1.bias',\n",
       " 'heads.object.decoder.0.cross_posembed.position_embedding_head.1.num_batches_tracked',\n",
       " 'heads.object.decoder.0.cross_posembed.position_embedding_head.1.running_mean',\n",
       " 'heads.object.decoder.0.cross_posembed.position_embedding_head.1.running_var',\n",
       " 'heads.object.decoder.0.cross_posembed.position_embedding_head.1.weight',\n",
       " 'heads.object.decoder.0.cross_posembed.position_embedding_head.3.bias',\n",
       " 'heads.object.decoder.0.cross_posembed.position_embedding_head.3.weight',\n",
       " 'heads.object.decoder.0.linear1.bias',\n",
       " 'heads.object.decoder.0.linear1.weight',\n",
       " 'heads.object.decoder.0.linear2.bias',\n",
       " 'heads.object.decoder.0.linear2.weight',\n",
       " 'heads.object.decoder.0.multihead_attn.in_proj_bias',\n",
       " 'heads.object.decoder.0.multihead_attn.in_proj_weight',\n",
       " 'heads.object.decoder.0.multihead_attn.out_proj.bias',\n",
       " 'heads.object.decoder.0.multihead_attn.out_proj.weight',\n",
       " 'heads.object.decoder.0.norm1.bias',\n",
       " 'heads.object.decoder.0.norm1.weight',\n",
       " 'heads.object.decoder.0.norm2.bias',\n",
       " 'heads.object.decoder.0.norm2.weight',\n",
       " 'heads.object.decoder.0.norm3.bias',\n",
       " 'heads.object.decoder.0.norm3.weight',\n",
       " 'heads.object.decoder.0.self_attn.in_proj_bias',\n",
       " 'heads.object.decoder.0.self_attn.in_proj_weight',\n",
       " 'heads.object.decoder.0.self_attn.out_proj.bias',\n",
       " 'heads.object.decoder.0.self_attn.out_proj.weight',\n",
       " 'heads.object.decoder.0.self_posembed.position_embedding_head.0.bias',\n",
       " 'heads.object.decoder.0.self_posembed.position_embedding_head.0.weight',\n",
       " 'heads.object.decoder.0.self_posembed.position_embedding_head.1.bias',\n",
       " 'heads.object.decoder.0.self_posembed.position_embedding_head.1.num_batches_tracked',\n",
       " 'heads.object.decoder.0.self_posembed.position_embedding_head.1.running_mean',\n",
       " 'heads.object.decoder.0.self_posembed.position_embedding_head.1.running_var',\n",
       " 'heads.object.decoder.0.self_posembed.position_embedding_head.1.weight',\n",
       " 'heads.object.decoder.0.self_posembed.position_embedding_head.3.bias',\n",
       " 'heads.object.decoder.0.self_posembed.position_embedding_head.3.weight',\n",
       " 'heads.object.heatmap_head.0.bn.bias',\n",
       " 'heads.object.heatmap_head.0.bn.num_batches_tracked',\n",
       " 'heads.object.heatmap_head.0.bn.running_mean',\n",
       " 'heads.object.heatmap_head.0.bn.running_var',\n",
       " 'heads.object.heatmap_head.0.bn.weight',\n",
       " 'heads.object.heatmap_head.0.conv.weight',\n",
       " 'heads.object.heatmap_head.1.bias',\n",
       " 'heads.object.heatmap_head.1.weight',\n",
       " 'heads.object.prediction_heads.0.center.0.bn.bias',\n",
       " 'heads.object.prediction_heads.0.center.0.bn.num_batches_tracked',\n",
       " 'heads.object.prediction_heads.0.center.0.bn.running_mean',\n",
       " 'heads.object.prediction_heads.0.center.0.bn.running_var',\n",
       " 'heads.object.prediction_heads.0.center.0.bn.weight',\n",
       " 'heads.object.prediction_heads.0.center.0.conv.weight',\n",
       " 'heads.object.prediction_heads.0.center.1.bias',\n",
       " 'heads.object.prediction_heads.0.center.1.weight',\n",
       " 'heads.object.prediction_heads.0.dim.0.bn.bias',\n",
       " 'heads.object.prediction_heads.0.dim.0.bn.num_batches_tracked',\n",
       " 'heads.object.prediction_heads.0.dim.0.bn.running_mean',\n",
       " 'heads.object.prediction_heads.0.dim.0.bn.running_var',\n",
       " 'heads.object.prediction_heads.0.dim.0.bn.weight',\n",
       " 'heads.object.prediction_heads.0.dim.0.conv.weight',\n",
       " 'heads.object.prediction_heads.0.dim.1.bias',\n",
       " 'heads.object.prediction_heads.0.dim.1.weight',\n",
       " 'heads.object.prediction_heads.0.heatmap.0.bn.bias',\n",
       " 'heads.object.prediction_heads.0.heatmap.0.bn.num_batches_tracked',\n",
       " 'heads.object.prediction_heads.0.heatmap.0.bn.running_mean',\n",
       " 'heads.object.prediction_heads.0.heatmap.0.bn.running_var',\n",
       " 'heads.object.prediction_heads.0.heatmap.0.bn.weight',\n",
       " 'heads.object.prediction_heads.0.heatmap.0.conv.weight',\n",
       " 'heads.object.prediction_heads.0.heatmap.1.bias',\n",
       " 'heads.object.prediction_heads.0.heatmap.1.weight',\n",
       " 'heads.object.prediction_heads.0.height.0.bn.bias',\n",
       " 'heads.object.prediction_heads.0.height.0.bn.num_batches_tracked',\n",
       " 'heads.object.prediction_heads.0.height.0.bn.running_mean',\n",
       " 'heads.object.prediction_heads.0.height.0.bn.running_var',\n",
       " 'heads.object.prediction_heads.0.height.0.bn.weight',\n",
       " 'heads.object.prediction_heads.0.height.0.conv.weight',\n",
       " 'heads.object.prediction_heads.0.height.1.bias',\n",
       " 'heads.object.prediction_heads.0.height.1.weight',\n",
       " 'heads.object.prediction_heads.0.rot.0.bn.bias',\n",
       " 'heads.object.prediction_heads.0.rot.0.bn.num_batches_tracked',\n",
       " 'heads.object.prediction_heads.0.rot.0.bn.running_mean',\n",
       " 'heads.object.prediction_heads.0.rot.0.bn.running_var',\n",
       " 'heads.object.prediction_heads.0.rot.0.bn.weight',\n",
       " 'heads.object.prediction_heads.0.rot.0.conv.weight',\n",
       " 'heads.object.prediction_heads.0.rot.1.bias',\n",
       " 'heads.object.prediction_heads.0.rot.1.weight',\n",
       " 'heads.object.prediction_heads.0.vel.0.bn.bias',\n",
       " 'heads.object.prediction_heads.0.vel.0.bn.num_batches_tracked',\n",
       " 'heads.object.prediction_heads.0.vel.0.bn.running_mean',\n",
       " 'heads.object.prediction_heads.0.vel.0.bn.running_var',\n",
       " 'heads.object.prediction_heads.0.vel.0.bn.weight',\n",
       " 'heads.object.prediction_heads.0.vel.0.conv.weight',\n",
       " 'heads.object.prediction_heads.0.vel.1.bias',\n",
       " 'heads.object.prediction_heads.0.vel.1.weight',\n",
       " 'heads.object.shared_conv.bias',\n",
       " 'heads.object.shared_conv.weight'}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi2seg_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f22ae8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_pth=torch.load(det_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d742e9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "det_key=multi_pth['state_dict'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d7820345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'heads.map.classifier.0.weight',\n",
       " 'heads.map.classifier.1.bias',\n",
       " 'heads.map.classifier.1.num_batches_tracked',\n",
       " 'heads.map.classifier.1.running_mean',\n",
       " 'heads.map.classifier.1.running_var',\n",
       " 'heads.map.classifier.1.weight',\n",
       " 'heads.map.classifier.3.weight',\n",
       " 'heads.map.classifier.4.bias',\n",
       " 'heads.map.classifier.4.num_batches_tracked',\n",
       " 'heads.map.classifier.4.running_mean',\n",
       " 'heads.map.classifier.4.running_var',\n",
       " 'heads.map.classifier.4.weight',\n",
       " 'heads.map.classifier.6.bias',\n",
       " 'heads.map.classifier.6.weight'}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_key-det_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "46cbcb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi2det_key=multi_key-det_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "cf8b5a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['encoders.camera.backbone.patch_embed.projection.weight', 'encoders.camera.backbone.patch_embed.projection.bias', 'encoders.camera.backbone.patch_embed.norm.weight', 'encoders.camera.backbone.patch_embed.norm.bias', 'encoders.camera.backbone.stages.0.blocks.0.norm1.weight', 'encoders.camera.backbone.stages.0.blocks.0.norm1.bias', 'encoders.camera.backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table', 'encoders.camera.backbone.stages.0.blocks.0.attn.w_msa.relative_position_index', 'encoders.camera.backbone.stages.0.blocks.0.attn.w_msa.qkv.weight', 'encoders.camera.backbone.stages.0.blocks.0.attn.w_msa.qkv.bias', 'encoders.camera.backbone.stages.0.blocks.0.attn.w_msa.proj.weight', 'encoders.camera.backbone.stages.0.blocks.0.attn.w_msa.proj.bias', 'encoders.camera.backbone.stages.0.blocks.0.norm2.weight', 'encoders.camera.backbone.stages.0.blocks.0.norm2.bias', 'encoders.camera.backbone.stages.0.blocks.0.ffn.layers.0.0.weight', 'encoders.camera.backbone.stages.0.blocks.0.ffn.layers.0.0.bias', 'encoders.camera.backbone.stages.0.blocks.0.ffn.layers.1.weight', 'encoders.camera.backbone.stages.0.blocks.0.ffn.layers.1.bias', 'encoders.camera.backbone.stages.0.blocks.1.norm1.weight', 'encoders.camera.backbone.stages.0.blocks.1.norm1.bias', 'encoders.camera.backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table', 'encoders.camera.backbone.stages.0.blocks.1.attn.w_msa.relative_position_index', 'encoders.camera.backbone.stages.0.blocks.1.attn.w_msa.qkv.weight', 'encoders.camera.backbone.stages.0.blocks.1.attn.w_msa.qkv.bias', 'encoders.camera.backbone.stages.0.blocks.1.attn.w_msa.proj.weight', 'encoders.camera.backbone.stages.0.blocks.1.attn.w_msa.proj.bias', 'encoders.camera.backbone.stages.0.blocks.1.norm2.weight', 'encoders.camera.backbone.stages.0.blocks.1.norm2.bias', 'encoders.camera.backbone.stages.0.blocks.1.ffn.layers.0.0.weight', 'encoders.camera.backbone.stages.0.blocks.1.ffn.layers.0.0.bias', 'encoders.camera.backbone.stages.0.blocks.1.ffn.layers.1.weight', 'encoders.camera.backbone.stages.0.blocks.1.ffn.layers.1.bias', 'encoders.camera.backbone.stages.0.downsample.norm.weight', 'encoders.camera.backbone.stages.0.downsample.norm.bias', 'encoders.camera.backbone.stages.0.downsample.reduction.weight', 'encoders.camera.backbone.stages.1.blocks.0.norm1.weight', 'encoders.camera.backbone.stages.1.blocks.0.norm1.bias', 'encoders.camera.backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table', 'encoders.camera.backbone.stages.1.blocks.0.attn.w_msa.relative_position_index', 'encoders.camera.backbone.stages.1.blocks.0.attn.w_msa.qkv.weight', 'encoders.camera.backbone.stages.1.blocks.0.attn.w_msa.qkv.bias', 'encoders.camera.backbone.stages.1.blocks.0.attn.w_msa.proj.weight', 'encoders.camera.backbone.stages.1.blocks.0.attn.w_msa.proj.bias', 'encoders.camera.backbone.stages.1.blocks.0.norm2.weight', 'encoders.camera.backbone.stages.1.blocks.0.norm2.bias', 'encoders.camera.backbone.stages.1.blocks.0.ffn.layers.0.0.weight', 'encoders.camera.backbone.stages.1.blocks.0.ffn.layers.0.0.bias', 'encoders.camera.backbone.stages.1.blocks.0.ffn.layers.1.weight', 'encoders.camera.backbone.stages.1.blocks.0.ffn.layers.1.bias', 'encoders.camera.backbone.stages.1.blocks.1.norm1.weight', 'encoders.camera.backbone.stages.1.blocks.1.norm1.bias', 'encoders.camera.backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table', 'encoders.camera.backbone.stages.1.blocks.1.attn.w_msa.relative_position_index', 'encoders.camera.backbone.stages.1.blocks.1.attn.w_msa.qkv.weight', 'encoders.camera.backbone.stages.1.blocks.1.attn.w_msa.qkv.bias', 'encoders.camera.backbone.stages.1.blocks.1.attn.w_msa.proj.weight', 'encoders.camera.backbone.stages.1.blocks.1.attn.w_msa.proj.bias', 'encoders.camera.backbone.stages.1.blocks.1.norm2.weight', 'encoders.camera.backbone.stages.1.blocks.1.norm2.bias', 'encoders.camera.backbone.stages.1.blocks.1.ffn.layers.0.0.weight', 'encoders.camera.backbone.stages.1.blocks.1.ffn.layers.0.0.bias', 'encoders.camera.backbone.stages.1.blocks.1.ffn.layers.1.weight', 'encoders.camera.backbone.stages.1.blocks.1.ffn.layers.1.bias', 'encoders.camera.backbone.stages.1.downsample.norm.weight', 'encoders.camera.backbone.stages.1.downsample.norm.bias', 'encoders.camera.backbone.stages.1.downsample.reduction.weight', 'encoders.camera.backbone.stages.2.blocks.0.norm1.weight', 'encoders.camera.backbone.stages.2.blocks.0.norm1.bias', 'encoders.camera.backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table', 'encoders.camera.backbone.stages.2.blocks.0.attn.w_msa.relative_position_index', 'encoders.camera.backbone.stages.2.blocks.0.attn.w_msa.qkv.weight', 'encoders.camera.backbone.stages.2.blocks.0.attn.w_msa.qkv.bias', 'encoders.camera.backbone.stages.2.blocks.0.attn.w_msa.proj.weight', 'encoders.camera.backbone.stages.2.blocks.0.attn.w_msa.proj.bias', 'encoders.camera.backbone.stages.2.blocks.0.norm2.weight', 'encoders.camera.backbone.stages.2.blocks.0.norm2.bias', 'encoders.camera.backbone.stages.2.blocks.0.ffn.layers.0.0.weight', 'encoders.camera.backbone.stages.2.blocks.0.ffn.layers.0.0.bias', 'encoders.camera.backbone.stages.2.blocks.0.ffn.layers.1.weight', 'encoders.camera.backbone.stages.2.blocks.0.ffn.layers.1.bias', 'encoders.camera.backbone.stages.2.blocks.1.norm1.weight', 'encoders.camera.backbone.stages.2.blocks.1.norm1.bias', 'encoders.camera.backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table', 'encoders.camera.backbone.stages.2.blocks.1.attn.w_msa.relative_position_index', 'encoders.camera.backbone.stages.2.blocks.1.attn.w_msa.qkv.weight', 'encoders.camera.backbone.stages.2.blocks.1.attn.w_msa.qkv.bias', 'encoders.camera.backbone.stages.2.blocks.1.attn.w_msa.proj.weight', 'encoders.camera.backbone.stages.2.blocks.1.attn.w_msa.proj.bias', 'encoders.camera.backbone.stages.2.blocks.1.norm2.weight', 'encoders.camera.backbone.stages.2.blocks.1.norm2.bias', 'encoders.camera.backbone.stages.2.blocks.1.ffn.layers.0.0.weight', 'encoders.camera.backbone.stages.2.blocks.1.ffn.layers.0.0.bias', 'encoders.camera.backbone.stages.2.blocks.1.ffn.layers.1.weight', 'encoders.camera.backbone.stages.2.blocks.1.ffn.layers.1.bias', 'encoders.camera.backbone.stages.2.blocks.2.norm1.weight', 'encoders.camera.backbone.stages.2.blocks.2.norm1.bias', 'encoders.camera.backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table', 'encoders.camera.backbone.stages.2.blocks.2.attn.w_msa.relative_position_index', 'encoders.camera.backbone.stages.2.blocks.2.attn.w_msa.qkv.weight', 'encoders.camera.backbone.stages.2.blocks.2.attn.w_msa.qkv.bias', 'encoders.camera.backbone.stages.2.blocks.2.attn.w_msa.proj.weight', 'encoders.camera.backbone.stages.2.blocks.2.attn.w_msa.proj.bias', 'encoders.camera.backbone.stages.2.blocks.2.norm2.weight', 'encoders.camera.backbone.stages.2.blocks.2.norm2.bias', 'encoders.camera.backbone.stages.2.blocks.2.ffn.layers.0.0.weight', 'encoders.camera.backbone.stages.2.blocks.2.ffn.layers.0.0.bias', 'encoders.camera.backbone.stages.2.blocks.2.ffn.layers.1.weight', 'encoders.camera.backbone.stages.2.blocks.2.ffn.layers.1.bias', 'encoders.camera.backbone.stages.2.blocks.3.norm1.weight', 'encoders.camera.backbone.stages.2.blocks.3.norm1.bias', 'encoders.camera.backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table', 'encoders.camera.backbone.stages.2.blocks.3.attn.w_msa.relative_position_index', 'encoders.camera.backbone.stages.2.blocks.3.attn.w_msa.qkv.weight', 'encoders.camera.backbone.stages.2.blocks.3.attn.w_msa.qkv.bias', 'encoders.camera.backbone.stages.2.blocks.3.attn.w_msa.proj.weight', 'encoders.camera.backbone.stages.2.blocks.3.attn.w_msa.proj.bias', 'encoders.camera.backbone.stages.2.blocks.3.norm2.weight', 'encoders.camera.backbone.stages.2.blocks.3.norm2.bias', 'encoders.camera.backbone.stages.2.blocks.3.ffn.layers.0.0.weight', 'encoders.camera.backbone.stages.2.blocks.3.ffn.layers.0.0.bias', 'encoders.camera.backbone.stages.2.blocks.3.ffn.layers.1.weight', 'encoders.camera.backbone.stages.2.blocks.3.ffn.layers.1.bias', 'encoders.camera.backbone.stages.2.blocks.4.norm1.weight', 'encoders.camera.backbone.stages.2.blocks.4.norm1.bias', 'encoders.camera.backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table', 'encoders.camera.backbone.stages.2.blocks.4.attn.w_msa.relative_position_index', 'encoders.camera.backbone.stages.2.blocks.4.attn.w_msa.qkv.weight', 'encoders.camera.backbone.stages.2.blocks.4.attn.w_msa.qkv.bias', 'encoders.camera.backbone.stages.2.blocks.4.attn.w_msa.proj.weight', 'encoders.camera.backbone.stages.2.blocks.4.attn.w_msa.proj.bias', 'encoders.camera.backbone.stages.2.blocks.4.norm2.weight', 'encoders.camera.backbone.stages.2.blocks.4.norm2.bias', 'encoders.camera.backbone.stages.2.blocks.4.ffn.layers.0.0.weight', 'encoders.camera.backbone.stages.2.blocks.4.ffn.layers.0.0.bias', 'encoders.camera.backbone.stages.2.blocks.4.ffn.layers.1.weight', 'encoders.camera.backbone.stages.2.blocks.4.ffn.layers.1.bias', 'encoders.camera.backbone.stages.2.blocks.5.norm1.weight', 'encoders.camera.backbone.stages.2.blocks.5.norm1.bias', 'encoders.camera.backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table', 'encoders.camera.backbone.stages.2.blocks.5.attn.w_msa.relative_position_index', 'encoders.camera.backbone.stages.2.blocks.5.attn.w_msa.qkv.weight', 'encoders.camera.backbone.stages.2.blocks.5.attn.w_msa.qkv.bias', 'encoders.camera.backbone.stages.2.blocks.5.attn.w_msa.proj.weight', 'encoders.camera.backbone.stages.2.blocks.5.attn.w_msa.proj.bias', 'encoders.camera.backbone.stages.2.blocks.5.norm2.weight', 'encoders.camera.backbone.stages.2.blocks.5.norm2.bias', 'encoders.camera.backbone.stages.2.blocks.5.ffn.layers.0.0.weight', 'encoders.camera.backbone.stages.2.blocks.5.ffn.layers.0.0.bias', 'encoders.camera.backbone.stages.2.blocks.5.ffn.layers.1.weight', 'encoders.camera.backbone.stages.2.blocks.5.ffn.layers.1.bias', 'encoders.camera.backbone.stages.2.downsample.norm.weight', 'encoders.camera.backbone.stages.2.downsample.norm.bias', 'encoders.camera.backbone.stages.2.downsample.reduction.weight', 'encoders.camera.backbone.stages.3.blocks.0.norm1.weight', 'encoders.camera.backbone.stages.3.blocks.0.norm1.bias', 'encoders.camera.backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table', 'encoders.camera.backbone.stages.3.blocks.0.attn.w_msa.relative_position_index', 'encoders.camera.backbone.stages.3.blocks.0.attn.w_msa.qkv.weight', 'encoders.camera.backbone.stages.3.blocks.0.attn.w_msa.qkv.bias', 'encoders.camera.backbone.stages.3.blocks.0.attn.w_msa.proj.weight', 'encoders.camera.backbone.stages.3.blocks.0.attn.w_msa.proj.bias', 'encoders.camera.backbone.stages.3.blocks.0.norm2.weight', 'encoders.camera.backbone.stages.3.blocks.0.norm2.bias', 'encoders.camera.backbone.stages.3.blocks.0.ffn.layers.0.0.weight', 'encoders.camera.backbone.stages.3.blocks.0.ffn.layers.0.0.bias', 'encoders.camera.backbone.stages.3.blocks.0.ffn.layers.1.weight', 'encoders.camera.backbone.stages.3.blocks.0.ffn.layers.1.bias', 'encoders.camera.backbone.stages.3.blocks.1.norm1.weight', 'encoders.camera.backbone.stages.3.blocks.1.norm1.bias', 'encoders.camera.backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table', 'encoders.camera.backbone.stages.3.blocks.1.attn.w_msa.relative_position_index', 'encoders.camera.backbone.stages.3.blocks.1.attn.w_msa.qkv.weight', 'encoders.camera.backbone.stages.3.blocks.1.attn.w_msa.qkv.bias', 'encoders.camera.backbone.stages.3.blocks.1.attn.w_msa.proj.weight', 'encoders.camera.backbone.stages.3.blocks.1.attn.w_msa.proj.bias', 'encoders.camera.backbone.stages.3.blocks.1.norm2.weight', 'encoders.camera.backbone.stages.3.blocks.1.norm2.bias', 'encoders.camera.backbone.stages.3.blocks.1.ffn.layers.0.0.weight', 'encoders.camera.backbone.stages.3.blocks.1.ffn.layers.0.0.bias', 'encoders.camera.backbone.stages.3.blocks.1.ffn.layers.1.weight', 'encoders.camera.backbone.stages.3.blocks.1.ffn.layers.1.bias', 'encoders.camera.backbone.norm1.weight', 'encoders.camera.backbone.norm1.bias', 'encoders.camera.backbone.norm2.weight', 'encoders.camera.backbone.norm2.bias', 'encoders.camera.backbone.norm3.weight', 'encoders.camera.backbone.norm3.bias', 'encoders.camera.neck.lateral_convs.0.conv.weight', 'encoders.camera.neck.lateral_convs.0.bn.weight', 'encoders.camera.neck.lateral_convs.0.bn.bias', 'encoders.camera.neck.lateral_convs.0.bn.running_mean', 'encoders.camera.neck.lateral_convs.0.bn.running_var', 'encoders.camera.neck.lateral_convs.0.bn.num_batches_tracked', 'encoders.camera.neck.lateral_convs.1.conv.weight', 'encoders.camera.neck.lateral_convs.1.bn.weight', 'encoders.camera.neck.lateral_convs.1.bn.bias', 'encoders.camera.neck.lateral_convs.1.bn.running_mean', 'encoders.camera.neck.lateral_convs.1.bn.running_var', 'encoders.camera.neck.lateral_convs.1.bn.num_batches_tracked', 'encoders.camera.neck.fpn_convs.0.conv.weight', 'encoders.camera.neck.fpn_convs.0.bn.weight', 'encoders.camera.neck.fpn_convs.0.bn.bias', 'encoders.camera.neck.fpn_convs.0.bn.running_mean', 'encoders.camera.neck.fpn_convs.0.bn.running_var', 'encoders.camera.neck.fpn_convs.0.bn.num_batches_tracked', 'encoders.camera.neck.fpn_convs.1.conv.weight', 'encoders.camera.neck.fpn_convs.1.bn.weight', 'encoders.camera.neck.fpn_convs.1.bn.bias', 'encoders.camera.neck.fpn_convs.1.bn.running_mean', 'encoders.camera.neck.fpn_convs.1.bn.running_var', 'encoders.camera.neck.fpn_convs.1.bn.num_batches_tracked', 'encoders.camera.vtransform.dx', 'encoders.camera.vtransform.bx', 'encoders.camera.vtransform.nx', 'encoders.camera.vtransform.frustum', 'encoders.camera.vtransform.dtransform.0.weight', 'encoders.camera.vtransform.dtransform.0.bias', 'encoders.camera.vtransform.dtransform.1.weight', 'encoders.camera.vtransform.dtransform.1.bias', 'encoders.camera.vtransform.dtransform.1.running_mean', 'encoders.camera.vtransform.dtransform.1.running_var', 'encoders.camera.vtransform.dtransform.1.num_batches_tracked', 'encoders.camera.vtransform.dtransform.3.weight', 'encoders.camera.vtransform.dtransform.3.bias', 'encoders.camera.vtransform.dtransform.4.weight', 'encoders.camera.vtransform.dtransform.4.bias', 'encoders.camera.vtransform.dtransform.4.running_mean', 'encoders.camera.vtransform.dtransform.4.running_var', 'encoders.camera.vtransform.dtransform.4.num_batches_tracked', 'encoders.camera.vtransform.dtransform.6.weight', 'encoders.camera.vtransform.dtransform.6.bias', 'encoders.camera.vtransform.dtransform.7.weight', 'encoders.camera.vtransform.dtransform.7.bias', 'encoders.camera.vtransform.dtransform.7.running_mean', 'encoders.camera.vtransform.dtransform.7.running_var', 'encoders.camera.vtransform.dtransform.7.num_batches_tracked', 'encoders.camera.vtransform.depth_map_transform.0.weight', 'encoders.camera.vtransform.depth_map_transform.0.bias', 'encoders.camera.vtransform.depth_map_transform.1.weight', 'encoders.camera.vtransform.depth_map_transform.1.bias', 'encoders.camera.vtransform.depth_map_transform.1.running_mean', 'encoders.camera.vtransform.depth_map_transform.1.running_var', 'encoders.camera.vtransform.depth_map_transform.1.num_batches_tracked', 'encoders.camera.vtransform.depth_map_transform.3.weight', 'encoders.camera.vtransform.depth_map_transform.3.bias', 'encoders.camera.vtransform.prenet.0.weight', 'encoders.camera.vtransform.prenet.0.bias', 'encoders.camera.vtransform.prenet.1.weight', 'encoders.camera.vtransform.prenet.1.bias', 'encoders.camera.vtransform.prenet.1.running_mean', 'encoders.camera.vtransform.prenet.1.running_var', 'encoders.camera.vtransform.prenet.1.num_batches_tracked', 'encoders.camera.vtransform.prenet.3.weight', 'encoders.camera.vtransform.prenet.3.bias', 'encoders.camera.vtransform.prenet.4.weight', 'encoders.camera.vtransform.prenet.4.bias', 'encoders.camera.vtransform.prenet.4.running_mean', 'encoders.camera.vtransform.prenet.4.running_var', 'encoders.camera.vtransform.prenet.4.num_batches_tracked', 'encoders.camera.vtransform.depthnet.weight', 'encoders.camera.vtransform.depthnet.bias', 'encoders.camera.vtransform.contextnet.weight', 'encoders.camera.vtransform.contextnet.bias', 'encoders.camera.vtransform.attention1.conv.weight', 'encoders.camera.vtransform.attention1.conv.bias', 'encoders.camera.vtransform.attention1.channel_attention.fc.0.weight', 'encoders.camera.vtransform.attention1.channel_attention.fc.0.bias', 'encoders.camera.vtransform.attention1.channel_attention.fc.2.weight', 'encoders.camera.vtransform.attention1.channel_attention.fc.2.bias', 'encoders.camera.vtransform.convnet.0.weight', 'encoders.camera.vtransform.convnet.0.bias', 'encoders.camera.vtransform.convnet.1.weight', 'encoders.camera.vtransform.convnet.1.bias', 'encoders.camera.vtransform.convnet.1.running_mean', 'encoders.camera.vtransform.convnet.1.running_var', 'encoders.camera.vtransform.convnet.1.num_batches_tracked', 'encoders.camera.vtransform.convnet.3.weight', 'encoders.camera.vtransform.convnet.3.bias', 'encoders.camera.vtransform.convnet.4.weight', 'encoders.camera.vtransform.convnet.4.bias', 'encoders.camera.vtransform.convnet.4.running_mean', 'encoders.camera.vtransform.convnet.4.running_var', 'encoders.camera.vtransform.convnet.4.num_batches_tracked', 'encoders.camera.vtransform.downsample.0.weight', 'encoders.camera.vtransform.downsample.1.weight', 'encoders.camera.vtransform.downsample.1.bias', 'encoders.camera.vtransform.downsample.1.running_mean', 'encoders.camera.vtransform.downsample.1.running_var', 'encoders.camera.vtransform.downsample.1.num_batches_tracked', 'encoders.camera.vtransform.downsample.3.weight', 'encoders.camera.vtransform.downsample.4.weight', 'encoders.camera.vtransform.downsample.4.bias', 'encoders.camera.vtransform.downsample.4.running_mean', 'encoders.camera.vtransform.downsample.4.running_var', 'encoders.camera.vtransform.downsample.4.num_batches_tracked', 'encoders.camera.vtransform.downsample.6.weight', 'encoders.camera.vtransform.downsample.7.weight', 'encoders.camera.vtransform.downsample.7.bias', 'encoders.camera.vtransform.downsample.7.running_mean', 'encoders.camera.vtransform.downsample.7.running_var', 'encoders.camera.vtransform.downsample.7.num_batches_tracked', 'encoders.lidar.backbone.conv_input.0.weight', 'encoders.lidar.backbone.conv_input.1.weight', 'encoders.lidar.backbone.conv_input.1.bias', 'encoders.lidar.backbone.conv_input.1.running_mean', 'encoders.lidar.backbone.conv_input.1.running_var', 'encoders.lidar.backbone.conv_input.1.num_batches_tracked', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.0.conv1.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.0.bn1.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.0.bn1.bias', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.0.bn1.running_mean', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.0.bn1.running_var', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.0.bn1.num_batches_tracked', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.0.conv2.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.0.bn2.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.0.bn2.bias', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.0.bn2.running_mean', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.0.bn2.running_var', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.0.bn2.num_batches_tracked', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.1.conv1.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.1.bn1.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.1.bn1.bias', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.1.bn1.running_mean', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.1.bn1.running_var', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.1.bn1.num_batches_tracked', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.1.conv2.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.1.bn2.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.1.bn2.bias', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.1.bn2.running_mean', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.1.bn2.running_var', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.1.bn2.num_batches_tracked', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.2.0.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.2.1.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.2.1.bias', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.2.1.running_mean', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.2.1.running_var', 'encoders.lidar.backbone.encoder_layers.encoder_layer1.2.1.num_batches_tracked', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.0.conv1.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.0.bn1.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.0.bn1.bias', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.0.bn1.running_mean', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.0.bn1.running_var', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.0.bn1.num_batches_tracked', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.0.conv2.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.0.bn2.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.0.bn2.bias', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.0.bn2.running_mean', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.0.bn2.running_var', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.0.bn2.num_batches_tracked', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.1.conv1.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.1.bn1.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.1.bn1.bias', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.1.bn1.running_mean', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.1.bn1.running_var', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.1.bn1.num_batches_tracked', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.1.conv2.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.1.bn2.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.1.bn2.bias', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.1.bn2.running_mean', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.1.bn2.running_var', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.1.bn2.num_batches_tracked', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.2.0.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.2.1.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.2.1.bias', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.2.1.running_mean', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.2.1.running_var', 'encoders.lidar.backbone.encoder_layers.encoder_layer2.2.1.num_batches_tracked', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.0.conv1.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.0.bn1.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.0.bn1.bias', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.0.bn1.running_mean', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.0.bn1.running_var', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.0.bn1.num_batches_tracked', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.0.conv2.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.0.bn2.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.0.bn2.bias', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.0.bn2.running_mean', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.0.bn2.running_var', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.0.bn2.num_batches_tracked', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.1.conv1.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.1.bn1.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.1.bn1.bias', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.1.bn1.running_mean', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.1.bn1.running_var', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.1.bn1.num_batches_tracked', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.1.conv2.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.1.bn2.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.1.bn2.bias', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.1.bn2.running_mean', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.1.bn2.running_var', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.1.bn2.num_batches_tracked', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.2.0.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.2.1.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.2.1.bias', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.2.1.running_mean', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.2.1.running_var', 'encoders.lidar.backbone.encoder_layers.encoder_layer3.2.1.num_batches_tracked', 'encoders.lidar.backbone.encoder_layers.encoder_layer4.0.conv1.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer4.0.bn1.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer4.0.bn1.bias', 'encoders.lidar.backbone.encoder_layers.encoder_layer4.0.bn1.running_mean', 'encoders.lidar.backbone.encoder_layers.encoder_layer4.0.bn1.running_var', 'encoders.lidar.backbone.encoder_layers.encoder_layer4.0.bn1.num_batches_tracked', 'encoders.lidar.backbone.encoder_layers.encoder_layer4.0.conv2.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer4.0.bn2.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer4.0.bn2.bias', 'encoders.lidar.backbone.encoder_layers.encoder_layer4.0.bn2.running_mean', 'encoders.lidar.backbone.encoder_layers.encoder_layer4.0.bn2.running_var', 'encoders.lidar.backbone.encoder_layers.encoder_layer4.0.bn2.num_batches_tracked', 'encoders.lidar.backbone.encoder_layers.encoder_layer4.1.conv1.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer4.1.bn1.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer4.1.bn1.bias', 'encoders.lidar.backbone.encoder_layers.encoder_layer4.1.bn1.running_mean', 'encoders.lidar.backbone.encoder_layers.encoder_layer4.1.bn1.running_var', 'encoders.lidar.backbone.encoder_layers.encoder_layer4.1.bn1.num_batches_tracked', 'encoders.lidar.backbone.encoder_layers.encoder_layer4.1.conv2.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer4.1.bn2.weight', 'encoders.lidar.backbone.encoder_layers.encoder_layer4.1.bn2.bias', 'encoders.lidar.backbone.encoder_layers.encoder_layer4.1.bn2.running_mean', 'encoders.lidar.backbone.encoder_layers.encoder_layer4.1.bn2.running_var', 'encoders.lidar.backbone.encoder_layers.encoder_layer4.1.bn2.num_batches_tracked', 'encoders.lidar.backbone.conv_out.0.weight', 'encoders.lidar.backbone.conv_out.1.weight', 'encoders.lidar.backbone.conv_out.1.bias', 'encoders.lidar.backbone.conv_out.1.running_mean', 'encoders.lidar.backbone.conv_out.1.running_var', 'encoders.lidar.backbone.conv_out.1.num_batches_tracked', 'fuser.0.weight', 'fuser.1.weight', 'fuser.1.bias', 'fuser.1.running_mean', 'fuser.1.running_var', 'fuser.1.num_batches_tracked', 'decoder.backbone.blocks.0.0.weight', 'decoder.backbone.blocks.0.1.weight', 'decoder.backbone.blocks.0.1.bias', 'decoder.backbone.blocks.0.1.running_mean', 'decoder.backbone.blocks.0.1.running_var', 'decoder.backbone.blocks.0.1.num_batches_tracked', 'decoder.backbone.blocks.0.3.weight', 'decoder.backbone.blocks.0.4.weight', 'decoder.backbone.blocks.0.4.bias', 'decoder.backbone.blocks.0.4.running_mean', 'decoder.backbone.blocks.0.4.running_var', 'decoder.backbone.blocks.0.4.num_batches_tracked', 'decoder.backbone.blocks.0.6.weight', 'decoder.backbone.blocks.0.7.weight', 'decoder.backbone.blocks.0.7.bias', 'decoder.backbone.blocks.0.7.running_mean', 'decoder.backbone.blocks.0.7.running_var', 'decoder.backbone.blocks.0.7.num_batches_tracked', 'decoder.backbone.blocks.0.9.weight', 'decoder.backbone.blocks.0.10.weight', 'decoder.backbone.blocks.0.10.bias', 'decoder.backbone.blocks.0.10.running_mean', 'decoder.backbone.blocks.0.10.running_var', 'decoder.backbone.blocks.0.10.num_batches_tracked', 'decoder.backbone.blocks.0.12.weight', 'decoder.backbone.blocks.0.13.weight', 'decoder.backbone.blocks.0.13.bias', 'decoder.backbone.blocks.0.13.running_mean', 'decoder.backbone.blocks.0.13.running_var', 'decoder.backbone.blocks.0.13.num_batches_tracked', 'decoder.backbone.blocks.0.15.weight', 'decoder.backbone.blocks.0.16.weight', 'decoder.backbone.blocks.0.16.bias', 'decoder.backbone.blocks.0.16.running_mean', 'decoder.backbone.blocks.0.16.running_var', 'decoder.backbone.blocks.0.16.num_batches_tracked', 'decoder.backbone.blocks.1.0.weight', 'decoder.backbone.blocks.1.1.weight', 'decoder.backbone.blocks.1.1.bias', 'decoder.backbone.blocks.1.1.running_mean', 'decoder.backbone.blocks.1.1.running_var', 'decoder.backbone.blocks.1.1.num_batches_tracked', 'decoder.backbone.blocks.1.3.weight', 'decoder.backbone.blocks.1.4.weight', 'decoder.backbone.blocks.1.4.bias', 'decoder.backbone.blocks.1.4.running_mean', 'decoder.backbone.blocks.1.4.running_var', 'decoder.backbone.blocks.1.4.num_batches_tracked', 'decoder.backbone.blocks.1.6.weight', 'decoder.backbone.blocks.1.7.weight', 'decoder.backbone.blocks.1.7.bias', 'decoder.backbone.blocks.1.7.running_mean', 'decoder.backbone.blocks.1.7.running_var', 'decoder.backbone.blocks.1.7.num_batches_tracked', 'decoder.backbone.blocks.1.9.weight', 'decoder.backbone.blocks.1.10.weight', 'decoder.backbone.blocks.1.10.bias', 'decoder.backbone.blocks.1.10.running_mean', 'decoder.backbone.blocks.1.10.running_var', 'decoder.backbone.blocks.1.10.num_batches_tracked', 'decoder.backbone.blocks.1.12.weight', 'decoder.backbone.blocks.1.13.weight', 'decoder.backbone.blocks.1.13.bias', 'decoder.backbone.blocks.1.13.running_mean', 'decoder.backbone.blocks.1.13.running_var', 'decoder.backbone.blocks.1.13.num_batches_tracked', 'decoder.backbone.blocks.1.15.weight', 'decoder.backbone.blocks.1.16.weight', 'decoder.backbone.blocks.1.16.bias', 'decoder.backbone.blocks.1.16.running_mean', 'decoder.backbone.blocks.1.16.running_var', 'decoder.backbone.blocks.1.16.num_batches_tracked', 'decoder.neck.deblocks.0.0.weight', 'decoder.neck.deblocks.0.1.weight', 'decoder.neck.deblocks.0.1.bias', 'decoder.neck.deblocks.0.1.running_mean', 'decoder.neck.deblocks.0.1.running_var', 'decoder.neck.deblocks.0.1.num_batches_tracked', 'decoder.neck.deblocks.1.0.weight', 'decoder.neck.deblocks.1.1.weight', 'decoder.neck.deblocks.1.1.bias', 'decoder.neck.deblocks.1.1.running_mean', 'decoder.neck.deblocks.1.1.running_var', 'decoder.neck.deblocks.1.1.num_batches_tracked', 'heads.object.shared_conv.weight', 'heads.object.shared_conv.bias', 'heads.object.heatmap_head.0.conv.weight', 'heads.object.heatmap_head.0.bn.weight', 'heads.object.heatmap_head.0.bn.bias', 'heads.object.heatmap_head.0.bn.running_mean', 'heads.object.heatmap_head.0.bn.running_var', 'heads.object.heatmap_head.0.bn.num_batches_tracked', 'heads.object.heatmap_head.1.weight', 'heads.object.heatmap_head.1.bias', 'heads.object.class_encoding.weight', 'heads.object.class_encoding.bias', 'heads.object.decoder.0.self_attn.in_proj_weight', 'heads.object.decoder.0.self_attn.in_proj_bias', 'heads.object.decoder.0.self_attn.out_proj.weight', 'heads.object.decoder.0.self_attn.out_proj.bias', 'heads.object.decoder.0.multihead_attn.in_proj_weight', 'heads.object.decoder.0.multihead_attn.in_proj_bias', 'heads.object.decoder.0.multihead_attn.out_proj.weight', 'heads.object.decoder.0.multihead_attn.out_proj.bias', 'heads.object.decoder.0.linear1.weight', 'heads.object.decoder.0.linear1.bias', 'heads.object.decoder.0.linear2.weight', 'heads.object.decoder.0.linear2.bias', 'heads.object.decoder.0.norm1.weight', 'heads.object.decoder.0.norm1.bias', 'heads.object.decoder.0.norm2.weight', 'heads.object.decoder.0.norm2.bias', 'heads.object.decoder.0.norm3.weight', 'heads.object.decoder.0.norm3.bias', 'heads.object.decoder.0.self_posembed.position_embedding_head.0.weight', 'heads.object.decoder.0.self_posembed.position_embedding_head.0.bias', 'heads.object.decoder.0.self_posembed.position_embedding_head.1.weight', 'heads.object.decoder.0.self_posembed.position_embedding_head.1.bias', 'heads.object.decoder.0.self_posembed.position_embedding_head.1.running_mean', 'heads.object.decoder.0.self_posembed.position_embedding_head.1.running_var', 'heads.object.decoder.0.self_posembed.position_embedding_head.1.num_batches_tracked', 'heads.object.decoder.0.self_posembed.position_embedding_head.3.weight', 'heads.object.decoder.0.self_posembed.position_embedding_head.3.bias', 'heads.object.decoder.0.cross_posembed.position_embedding_head.0.weight', 'heads.object.decoder.0.cross_posembed.position_embedding_head.0.bias', 'heads.object.decoder.0.cross_posembed.position_embedding_head.1.weight', 'heads.object.decoder.0.cross_posembed.position_embedding_head.1.bias', 'heads.object.decoder.0.cross_posembed.position_embedding_head.1.running_mean', 'heads.object.decoder.0.cross_posembed.position_embedding_head.1.running_var', 'heads.object.decoder.0.cross_posembed.position_embedding_head.1.num_batches_tracked', 'heads.object.decoder.0.cross_posembed.position_embedding_head.3.weight', 'heads.object.decoder.0.cross_posembed.position_embedding_head.3.bias', 'heads.object.prediction_heads.0.center.0.conv.weight', 'heads.object.prediction_heads.0.center.0.bn.weight', 'heads.object.prediction_heads.0.center.0.bn.bias', 'heads.object.prediction_heads.0.center.0.bn.running_mean', 'heads.object.prediction_heads.0.center.0.bn.running_var', 'heads.object.prediction_heads.0.center.0.bn.num_batches_tracked', 'heads.object.prediction_heads.0.center.1.weight', 'heads.object.prediction_heads.0.center.1.bias', 'heads.object.prediction_heads.0.height.0.conv.weight', 'heads.object.prediction_heads.0.height.0.bn.weight', 'heads.object.prediction_heads.0.height.0.bn.bias', 'heads.object.prediction_heads.0.height.0.bn.running_mean', 'heads.object.prediction_heads.0.height.0.bn.running_var', 'heads.object.prediction_heads.0.height.0.bn.num_batches_tracked', 'heads.object.prediction_heads.0.height.1.weight', 'heads.object.prediction_heads.0.height.1.bias', 'heads.object.prediction_heads.0.dim.0.conv.weight', 'heads.object.prediction_heads.0.dim.0.bn.weight', 'heads.object.prediction_heads.0.dim.0.bn.bias', 'heads.object.prediction_heads.0.dim.0.bn.running_mean', 'heads.object.prediction_heads.0.dim.0.bn.running_var', 'heads.object.prediction_heads.0.dim.0.bn.num_batches_tracked', 'heads.object.prediction_heads.0.dim.1.weight', 'heads.object.prediction_heads.0.dim.1.bias', 'heads.object.prediction_heads.0.rot.0.conv.weight', 'heads.object.prediction_heads.0.rot.0.bn.weight', 'heads.object.prediction_heads.0.rot.0.bn.bias', 'heads.object.prediction_heads.0.rot.0.bn.running_mean', 'heads.object.prediction_heads.0.rot.0.bn.running_var', 'heads.object.prediction_heads.0.rot.0.bn.num_batches_tracked', 'heads.object.prediction_heads.0.rot.1.weight', 'heads.object.prediction_heads.0.rot.1.bias', 'heads.object.prediction_heads.0.vel.0.conv.weight', 'heads.object.prediction_heads.0.vel.0.bn.weight', 'heads.object.prediction_heads.0.vel.0.bn.bias', 'heads.object.prediction_heads.0.vel.0.bn.running_mean', 'heads.object.prediction_heads.0.vel.0.bn.running_var', 'heads.object.prediction_heads.0.vel.0.bn.num_batches_tracked', 'heads.object.prediction_heads.0.vel.1.weight', 'heads.object.prediction_heads.0.vel.1.bias', 'heads.object.prediction_heads.0.heatmap.0.conv.weight', 'heads.object.prediction_heads.0.heatmap.0.bn.weight', 'heads.object.prediction_heads.0.heatmap.0.bn.bias', 'heads.object.prediction_heads.0.heatmap.0.bn.running_mean', 'heads.object.prediction_heads.0.heatmap.0.bn.running_var', 'heads.object.prediction_heads.0.heatmap.0.bn.num_batches_tracked', 'heads.object.prediction_heads.0.heatmap.1.weight', 'heads.object.prediction_heads.0.heatmap.1.bias'])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "det_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "931e0e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoders.camera.vtransform.attention1.conv.weight\n",
      "encoders.camera.vtransform.attention1.conv.bias\n",
      "encoders.camera.vtransform.attention1.channel_attention.fc.0.weight\n",
      "encoders.camera.vtransform.attention1.channel_attention.fc.0.bias\n",
      "encoders.camera.vtransform.attention1.channel_attention.fc.2.weight\n",
      "encoders.camera.vtransform.attention1.channel_attention.fc.2.bias\n"
     ]
    }
   ],
   "source": [
    "for i in det_key:\n",
    "    if i[:30]==\"encoders.camera.vtransform.att\":\n",
    "        print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "41e8e33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in seg_key:\n",
    "    if i[:30]==\"encoders.camera.vtransform.att\":\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0a9ab6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoders.camera.vtransform.attention1.conv.weight\n",
      "encoders.camera.vtransform.attention1.conv.bias\n",
      "encoders.camera.vtransform.attention1.channel_attention.fc.0.weight\n",
      "encoders.camera.vtransform.attention1.channel_attention.fc.0.bias\n",
      "encoders.camera.vtransform.attention1.channel_attention.fc.2.weight\n",
      "encoders.camera.vtransform.attention1.channel_attention.fc.2.bias\n"
     ]
    }
   ],
   "source": [
    "for i in multi_key:\n",
    "    if i[:30]==\"encoders.camera.vtransform.att\":\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b9b051e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "det_pth=torch.load(det_filepath)\n",
    "seg_pth=torch.load(seg_filepath)\n",
    "new_pth=det_pth\n",
    "for i in multi2det_key:\n",
    "    new_pth['state_dict'][i]=seg_pth['state_dict'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a558e921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heads.map.classifier.0.weight\n",
      "heads.map.classifier.1.weight\n",
      "heads.map.classifier.1.bias\n",
      "heads.map.classifier.1.running_mean\n",
      "heads.map.classifier.1.running_var\n",
      "heads.map.classifier.1.num_batches_tracked\n",
      "heads.map.classifier.3.weight\n",
      "heads.map.classifier.4.weight\n",
      "heads.map.classifier.4.bias\n",
      "heads.map.classifier.4.running_mean\n",
      "heads.map.classifier.4.running_var\n",
      "heads.map.classifier.4.num_batches_tracked\n",
      "heads.map.classifier.6.weight\n",
      "heads.map.classifier.6.bias\n"
     ]
    }
   ],
   "source": [
    "for i in seg_key:\n",
    "    if i[:20]==\"heads.map.classifier\":\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "c29513e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['meta', 'state_dict', 'optimizer'])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_pth.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2b741290",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi2new_key=multi_key-new_pth['state_dict'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "fcc9ba2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi2new_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a934a9a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_pth['state_dict'].keys()-multi_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a296c277",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[149], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_pth\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/data/workdirs/bevfusion-multitask/det_plus_seg/new.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/bevfusion-mit/lib/python3.8/site-packages/torch/serialization.py:380\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(opened_file) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    379\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[0;32m--> 380\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    381\u001b[0m _legacy_save(obj, opened_file, pickle_module, pickle_protocol)\n",
      "File \u001b[0;32m~/anaconda3/envs/bevfusion-mit/lib/python3.8/site-packages/torch/serialization.py:214\u001b[0m, in \u001b[0;36m_open_file.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m--> 214\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_like\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.save(new_pth,\"/data/workdirs/bevfusion-multitask/det_plus_seg/new.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9c0639",
   "metadata": {},
   "source": [
    "# det+seg_head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec895b1a",
   "metadata": {},
   "source": [
    "mAP: 0.6942                              \n",
    "mATE: 0.2855\n",
    "mASE: 0.2538\n",
    "mAOE: 0.3065\n",
    "mAVE: 0.2620\n",
    "mAAE: 0.1842\n",
    "NDS: 0.7179\n",
    "Eval time: 97.8s\n",
    "\n",
    "Per-class results:\n",
    "Object Class            AP      ATE     ASE     AOE     AVE     AAE   \n",
    "car                     0.893   0.169   0.149   0.057   0.282   0.186 \n",
    "truck                   0.653   0.318   0.180   0.080   0.255   0.223 \n",
    "bus                     0.767   0.326   0.185   0.061   0.429   0.268 \n",
    "trailer                 0.439   0.520   0.202   0.551   0.230   0.116 \n",
    "construction_vehicle    0.309   0.738   0.432   0.910   0.116   0.290 \n",
    "pedestrian              0.888   0.132   0.286   0.391   0.220   0.101 \n",
    "motorcycle              0.796   0.184   0.256   0.231   0.363   0.274 \n",
    "bicycle                 0.653   0.166   0.254   0.423   0.199   0.015 \n",
    "traffic_cone            0.800   0.122   0.322   nan     nan     nan   \n",
    "barrier                 0.743   0.178   0.273   0.056   nan     nan   \n",
    "{'map/drivable_area/iou@max': 0.10612747073173523, 'map/drivable_area/iou@0.35': 0.10612747073173523, 'map/drivable_area/iou@0.40': 0.03845725208520889, 'map/drivable_area/iou@0.45': 0.014784050174057484, 'map/drivable_area/iou@0.50': 0.0025197567883878946, 'map/drivable_area/iou@0.55': 1.8233087757835165e-05, 'map/drivable_area/iou@0.60': 4.589395302900812e-06, 'map/drivable_area/iou@0.65': 5.911773541811272e-07, 'map/ped_crossing/iou@max': 0.0008120648562908173, 'map/ped_crossing/iou@0.35': 0.0008120648562908173, 'map/ped_crossing/iou@0.40': 9.035829862114042e-05, 'map/ped_crossing/iou@0.45': 0.0, 'map/ped_crossing/iou@0.50': 0.0, 'map/ped_crossing/iou@0.55': 0.0, 'map/ped_crossing/iou@0.60': 0.0, 'map/ped_crossing/iou@0.65': 0.0, 'map/walkway/iou@max': 5.443493137136102e-05, 'map/walkway/iou@0.35': 5.443493137136102e-05, 'map/walkway/iou@0.40': 0.0, 'map/walkway/iou@0.45': 0.0, 'map/walkway/iou@0.50': 0.0, 'map/walkway/iou@0.55': 0.0, 'map/walkway/iou@0.60': 0.0, 'map/walkway/iou@0.65': 0.0, 'map/stop_line/iou@max': 0.0, 'map/stop_line/iou@0.35': 0.0, 'map/stop_line/iou@0.40': 0.0, 'map/stop_line/iou@0.45': 0.0, 'map/stop_line/iou@0.50': 0.0, 'map/stop_line/iou@0.55': 0.0, 'map/stop_line/iou@0.60': 0.0, 'map/stop_line/iou@0.65': 0.0, 'map/carpark_area/iou@max': 0.0, 'map/carpark_area/iou@0.35': 0.0, 'map/carpark_area/iou@0.40': 0.0, 'map/carpark_area/iou@0.45': 0.0, 'map/carpark_area/iou@0.50': 0.0, 'map/carpark_area/iou@0.55': 0.0, 'map/carpark_area/iou@0.60': 0.0, 'map/carpark_area/iou@0.65': 0.0, 'map/divider/iou@max': 0.0, 'map/divider/iou@0.35': 0.0, 'map/divider/iou@0.40': 0.0, 'map/divider/iou@0.45': 0.0, 'map/divider/iou@0.50': 0.0, 'map/divider/iou@0.55': 0.0, 'map/divider/iou@0.60': 0.0, 'map/divider/iou@0.65': 0.0, 'map/mean/iou@max': 0.017832329496741295, 'object/car_ap_dist_0.5': 0.8104, 'object/car_ap_dist_1.0': 0.8985, 'object/car_ap_dist_2.0': 0.9274, 'object/car_ap_dist_4.0': 0.9368, 'object/car_trans_err': 0.1695, 'object/car_scale_err': 0.1489, 'object/car_orient_err': 0.0567, 'object/car_vel_err': 0.2824, 'object/car_attr_err': 0.1864, 'object/mATE': 0.2855, 'object/mASE': 0.2538, 'object/mAOE': 0.3065, 'object/mAVE': 0.262, 'object/mAAE': 0.1842, 'object/truck_ap_dist_0.5': 0.4457, 'object/truck_ap_dist_1.0': 0.6538, 'object/truck_ap_dist_2.0': 0.7408, 'object/truck_ap_dist_4.0': 0.7734, 'object/truck_trans_err': 0.3181, 'object/truck_scale_err': 0.1795, 'object/truck_orient_err': 0.0797, 'object/truck_vel_err': 0.2553, 'object/truck_attr_err': 0.2228, 'object/construction_vehicle_ap_dist_0.5': 0.0474, 'object/construction_vehicle_ap_dist_1.0': 0.2123, 'object/construction_vehicle_ap_dist_2.0': 0.4344, 'object/construction_vehicle_ap_dist_4.0': 0.5434, 'object/construction_vehicle_trans_err': 0.7381, 'object/construction_vehicle_scale_err': 0.4324, 'object/construction_vehicle_orient_err': 0.91, 'object/construction_vehicle_vel_err': 0.1165, 'object/construction_vehicle_attr_err': 0.2905, 'object/bus_ap_dist_0.5': 0.5022, 'object/bus_ap_dist_1.0': 0.7774, 'object/bus_ap_dist_2.0': 0.8818, 'object/bus_ap_dist_4.0': 0.906, 'object/bus_trans_err': 0.3264, 'object/bus_scale_err': 0.1851, 'object/bus_orient_err': 0.0606, 'object/bus_vel_err': 0.4285, 'object/bus_attr_err': 0.2678, 'object/trailer_ap_dist_0.5': 0.1633, 'object/trailer_ap_dist_1.0': 0.3726, 'object/trailer_ap_dist_2.0': 0.5617, 'object/trailer_ap_dist_4.0': 0.6586, 'object/trailer_trans_err': 0.52, 'object/trailer_scale_err': 0.2017, 'object/trailer_orient_err': 0.5512, 'object/trailer_vel_err': 0.2304, 'object/trailer_attr_err': 0.1165, 'object/barrier_ap_dist_0.5': 0.6541, 'object/barrier_ap_dist_1.0': 0.7406, 'object/barrier_ap_dist_2.0': 0.784, 'object/barrier_ap_dist_4.0': 0.7948, 'object/barrier_trans_err': 0.1779, 'object/barrier_scale_err': 0.2727, 'object/barrier_orient_err': 0.056, 'object/barrier_vel_err': nan, 'object/barrier_attr_err': nan, 'object/motorcycle_ap_dist_0.5': 0.699, 'object/motorcycle_ap_dist_1.0': 0.8128, 'object/motorcycle_ap_dist_2.0': 0.8294, 'object/motorcycle_ap_dist_4.0': 0.8413, 'object/motorcycle_trans_err': 0.1841, 'object/motorcycle_scale_err': 0.2561, 'object/motorcycle_orient_err': 0.2308, 'object/motorcycle_vel_err': 0.3629, 'object/motorcycle_attr_err': 0.2737, 'object/bicycle_ap_dist_0.5': 0.6143, 'object/bicycle_ap_dist_1.0': 0.6528, 'object/bicycle_ap_dist_2.0': 0.6659, 'object/bicycle_ap_dist_4.0': 0.6806, 'object/bicycle_trans_err': 0.1663, 'object/bicycle_scale_err': 0.2536, 'object/bicycle_orient_err': 0.4226, 'object/bicycle_vel_err': 0.1993, 'object/bicycle_attr_err': 0.0151, 'object/pedestrian_ap_dist_0.5': 0.8707, 'object/pedestrian_ap_dist_1.0': 0.8845, 'object/pedestrian_ap_dist_2.0': 0.8943, 'object/pedestrian_ap_dist_4.0': 0.9035, 'object/pedestrian_trans_err': 0.1322, 'object/pedestrian_scale_err': 0.2856, 'object/pedestrian_orient_err': 0.3908, 'object/pedestrian_vel_err': 0.2204, 'object/pedestrian_attr_err': 0.1009, 'object/traffic_cone_ap_dist_0.5': 0.7788, 'object/traffic_cone_ap_dist_1.0': 0.788, 'object/traffic_cone_ap_dist_2.0': 0.8051, 'object/traffic_cone_ap_dist_4.0': 0.8268, 'object/traffic_cone_trans_err': 0.1224, 'object/traffic_cone_scale_err': 0.3218, 'object/traffic_cone_orient_err': nan, 'object/traffic_cone_vel_err': nan, 'object/traffic_cone_attr_err': nan, 'object/nds': 0.7179240888818436, 'object/map': 0.6942315554018366}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b134eb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heads.map.classifier.1.running_mean\n",
      "heads.map.classifier.1.bias\n",
      "heads.map.classifier.4.weight\n",
      "heads.map.classifier.4.running_var\n",
      "heads.map.classifier.4.running_mean\n",
      "heads.map.classifier.1.weight\n",
      "heads.map.classifier.6.bias\n",
      "heads.map.classifier.6.weight\n",
      "heads.map.classifier.1.num_batches_tracked\n",
      "heads.map.classifier.0.weight\n",
      "heads.map.classifier.4.bias\n",
      "heads.map.classifier.1.running_var\n",
      "heads.map.classifier.3.weight\n",
      "heads.map.classifier.4.num_batches_tracked\n"
     ]
    }
   ],
   "source": [
    "det_pth=torch.load(det_filepath)\n",
    "seg_pth=torch.load(seg_filepath)\n",
    "new_pth2=torch.load(multi_filepath)\n",
    "for i in multi2det_key:\n",
    "    print(i)\n",
    "    new_pth['state_dict'][i]=seg_pth['state_dict'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "640529e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_pth2['state_dict'].keys()-multi_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "524437ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_key-new_pth2['state_dict'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "b1cd2e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(new_pth2,\"/data/workdirs/bevfusion-multitask/det_plus_seg/new_multi_seg.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2386f9",
   "metadata": {},
   "source": [
    "# multi _ seg\n",
    "mAP: 0.6431                              \n",
    "mATE: 0.3074\n",
    "mASE: 0.2686\n",
    "mAOE: 0.4349\n",
    "mAVE: 0.3971\n",
    "mAAE: 0.1833\n",
    "NDS: 0.6624\n",
    "Eval time: 108.8s\n",
    "\n",
    "Per-class results:\n",
    "Object Class            AP      ATE     ASE     AOE     AVE     AAE   \n",
    "car                     0.887   0.177   0.155   0.064   0.342   0.189 \n",
    "truck                   0.500   0.378   0.230   0.130   0.370   0.247 \n",
    "bus                     0.699   0.377   0.193   0.135   0.830   0.270 \n",
    "trailer                 0.393   0.559   0.224   0.553   0.452   0.112 \n",
    "construction_vehicle    0.278   0.664   0.448   1.082   0.145   0.311 \n",
    "pedestrian              0.865   0.145   0.285   0.418   0.259   0.104 \n",
    "motorcycle              0.714   0.231   0.255   0.616   0.542   0.222 \n",
    "bicycle                 0.586   0.206   0.260   0.827   0.238   0.012 \n",
    "traffic_cone            0.768   0.143   0.356   nan     nan     nan   \n",
    "barrier                 0.743   0.195   0.279   0.089   nan     nan   \n",
    "{'map/drivable_area/iou@max': 0.7266930937767029, 'map/drivable_area/iou@0.35': 0.6615170240402222, 'map/drivable_area/iou@0.40': 0.7036038041114807, 'map/drivable_area/iou@0.45': 0.7266930937767029, 'map/drivable_area/iou@0.50': 0.7262260913848877, 'map/drivable_area/iou@0.55': 0.7023627161979675, 'map/drivable_area/iou@0.60': 0.6574963927268982, 'map/drivable_area/iou@0.65': 0.5964158177375793, 'map/ped_crossing/iou@max': 0.27256911993026733, 'map/ped_crossing/iou@0.35': 0.2625751495361328, 'map/ped_crossing/iou@0.40': 0.27256911993026733, 'map/ped_crossing/iou@0.45': 0.22742685675621033, 'map/ped_crossing/iou@0.50': 0.158382385969162, 'map/ped_crossing/iou@0.55': 0.09839142858982086, 'map/ped_crossing/iou@0.60': 0.05571597069501877, 'map/ped_crossing/iou@0.65': 0.027566680684685707, 'map/walkway/iou@max': 0.42948204278945923, 'map/walkway/iou@0.35': 0.3955197334289551, 'map/walkway/iou@0.40': 0.42948204278945923, 'map/walkway/iou@0.45': 0.42370954155921936, 'map/walkway/iou@0.50': 0.37246641516685486, 'map/walkway/iou@0.55': 0.2916600704193115, 'map/walkway/iou@0.60': 0.2064180225133896, 'map/walkway/iou@0.65': 0.13019852340221405, 'map/stop_line/iou@max': 0.17121027410030365, 'map/stop_line/iou@0.35': 0.17121027410030365, 'map/stop_line/iou@0.40': 0.12009777128696442, 'map/stop_line/iou@0.45': 0.05726191774010658, 'map/stop_line/iou@0.50': 0.027837667614221573, 'map/stop_line/iou@0.55': 0.013150770217180252, 'map/stop_line/iou@0.60': 0.005586232524365187, 'map/stop_line/iou@0.65': 0.002195988316088915, 'map/carpark_area/iou@max': 0.2904696762561798, 'map/carpark_area/iou@0.35': 0.2904696762561798, 'map/carpark_area/iou@0.40': 0.27111464738845825, 'map/carpark_area/iou@0.45': 0.23271334171295166, 'map/carpark_area/iou@0.50': 0.18425661325454712, 'map/carpark_area/iou@0.55': 0.13407665491104126, 'map/carpark_area/iou@0.60': 0.08946408331394196, 'map/carpark_area/iou@0.65': 0.052107423543930054, 'map/divider/iou@max': 0.2772532105445862, 'map/divider/iou@0.35': 0.27391090989112854, 'map/divider/iou@0.40': 0.2772532105445862, 'map/divider/iou@0.45': 0.2241506427526474, 'map/divider/iou@0.50': 0.14946283400058746, 'map/divider/iou@0.55': 0.08533210307359695, 'map/divider/iou@0.60': 0.04526831582188606, 'map/divider/iou@0.65': 0.021263672038912773, 'map/mean/iou@max': 0.36127957701683044, 'object/car_ap_dist_0.5': 0.797, 'object/car_ap_dist_1.0': 0.8939, 'object/car_ap_dist_2.0': 0.9226, 'object/car_ap_dist_4.0': 0.9333, 'object/car_trans_err': 0.1774, 'object/car_scale_err': 0.1555, 'object/car_orient_err': 0.0637, 'object/car_vel_err': 0.3424, 'object/car_attr_err': 0.1888, 'object/mATE': 0.3074, 'object/mASE': 0.2686, 'object/mAOE': 0.4349, 'object/mAVE': 0.3971, 'object/mAAE': 0.1833, 'object/truck_ap_dist_0.5': 0.2978, 'object/truck_ap_dist_1.0': 0.4875, 'object/truck_ap_dist_2.0': 0.5796, 'object/truck_ap_dist_4.0': 0.6343, 'object/truck_trans_err': 0.3776, 'object/truck_scale_err': 0.2303, 'object/truck_orient_err': 0.13, 'object/truck_vel_err': 0.3696, 'object/truck_attr_err': 0.2468, 'object/construction_vehicle_ap_dist_0.5': 0.0281, 'object/construction_vehicle_ap_dist_1.0': 0.2257, 'object/construction_vehicle_ap_dist_2.0': 0.3691, 'object/construction_vehicle_ap_dist_4.0': 0.4879, 'object/construction_vehicle_trans_err': 0.6635, 'object/construction_vehicle_scale_err': 0.4481, 'object/construction_vehicle_orient_err': 1.0819, 'object/construction_vehicle_vel_err': 0.1455, 'object/construction_vehicle_attr_err': 0.3114, 'object/bus_ap_dist_0.5': 0.4075, 'object/bus_ap_dist_1.0': 0.6864, 'object/bus_ap_dist_2.0': 0.829, 'object/bus_ap_dist_4.0': 0.8749, 'object/bus_trans_err': 0.3766, 'object/bus_scale_err': 0.1932, 'object/bus_orient_err': 0.1354, 'object/bus_vel_err': 0.8295, 'object/bus_attr_err': 0.2697, 'object/trailer_ap_dist_0.5': 0.0938, 'object/trailer_ap_dist_1.0': 0.3436, 'object/trailer_ap_dist_2.0': 0.5155, 'object/trailer_ap_dist_4.0': 0.6172, 'object/trailer_trans_err': 0.5589, 'object/trailer_scale_err': 0.2239, 'object/trailer_orient_err': 0.5527, 'object/trailer_vel_err': 0.4519, 'object/trailer_attr_err': 0.112, 'object/barrier_ap_dist_0.5': 0.629, 'object/barrier_ap_dist_1.0': 0.7443, 'object/barrier_ap_dist_2.0': 0.791, 'object/barrier_ap_dist_4.0': 0.8058, 'object/barrier_trans_err': 0.1954, 'object/barrier_scale_err': 0.279, 'object/barrier_orient_err': 0.0889, 'object/barrier_vel_err': nan, 'object/barrier_attr_err': nan, 'object/motorcycle_ap_dist_0.5': 0.5766, 'object/motorcycle_ap_dist_1.0': 0.742, 'object/motorcycle_ap_dist_2.0': 0.7628, 'object/motorcycle_ap_dist_4.0': 0.774, 'object/motorcycle_trans_err': 0.2305, 'object/motorcycle_scale_err': 0.2554, 'object/motorcycle_orient_err': 0.6161, 'object/motorcycle_vel_err': 0.5415, 'object/motorcycle_attr_err': 0.2222, 'object/bicycle_ap_dist_0.5': 0.5207, 'object/bicycle_ap_dist_1.0': 0.5944, 'object/bicycle_ap_dist_2.0': 0.6046, 'object/bicycle_ap_dist_4.0': 0.6243, 'object/bicycle_trans_err': 0.206, 'object/bicycle_scale_err': 0.2599, 'object/bicycle_orient_err': 0.8273, 'object/bicycle_vel_err': 0.238, 'object/bicycle_attr_err': 0.0124, 'object/pedestrian_ap_dist_0.5': 0.845, 'object/pedestrian_ap_dist_1.0': 0.8597, 'object/pedestrian_ap_dist_2.0': 0.8721, 'object/pedestrian_ap_dist_4.0': 0.8837, 'object/pedestrian_trans_err': 0.1453, 'object/pedestrian_scale_err': 0.285, 'object/pedestrian_orient_err': 0.4183, 'object/pedestrian_vel_err': 0.2588, 'object/pedestrian_attr_err': 0.1035, 'object/traffic_cone_ap_dist_0.5': 0.7383, 'object/traffic_cone_ap_dist_1.0': 0.7551, 'object/traffic_cone_ap_dist_2.0': 0.776, 'object/traffic_cone_ap_dist_4.0': 0.8012, 'object/traffic_cone_trans_err': 0.1425, 'object/traffic_cone_scale_err': 0.3559, 'object/traffic_cone_orient_err': nan, 'object/traffic_cone_vel_err': nan, 'object/traffic_cone_attr_err': nan, 'object/nds': 0.6624315902156354, 'object/map': 0.6431353363336558}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "86ded84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "det_pth=torch.load(det_filepath)\n",
    "seg_pth=torch.load(seg_filepath)\n",
    "new_pth3=seg_pth\n",
    "for i in multi2seg_key:\n",
    "    new_pth3['state_dict'][i]=det_pth['state_dict'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "f4c305e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(new_pth3,\"/data/workdirs/bevfusion-multitask/det_plus_seg/new_seg_det.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b564d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
